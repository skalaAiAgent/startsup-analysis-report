# tech state # ê¸°ìˆ  ë¬¸ì„œ ìš”ì•½ ë° í•µì‹¬ ê¸°ìˆ  ë„ì¶œì„ ìˆ˜í–‰í•˜ëŠ” Agentimport os
from typing import TypedDict, List, Dict, Annotated
from operator import add
from pathlib import Path

from langchain_openai import ChatOpenAI
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain.prompts import ChatPromptTemplate

from langgraph.graph import StateGraph, END
import numpy as np
import json
import requests
from bs4 import BeautifulSoup
from urllib.parse import quote
import time
from dotenv import load_dotenv

load_dotenv()

# ëª¨ë¸ ì´ˆê¸°í™”
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)  # ì ìˆ˜ ë‹¤ì–‘ì„±ì„ ìœ„í•´ temperature ì¦ê°€
embeddings = OllamaEmbeddings(model="nomic-embed-text")  # í˜„ì¬ ì„¤ì¹˜ëœ ì„ë² ë”© ëª¨ë¸

# PDF ë°ì´í„° ê²½ë¡œ
PDF_DATA_PATH = Path(r"C:\workspace\demo-app\skala_gai\data")

class TechState(TypedDict):
    """ì—ì´ì „íŠ¸ì˜ ìƒíƒœë¥¼ ì •ì˜í•˜ëŠ” í´ë˜ìŠ¤"""
    startup_names: List[str]  # í‰ê°€í•  ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 5ê°œ)
    current_startup: str  # í˜„ì¬ í‰ê°€ ì¤‘ì¸ ìŠ¤íƒ€íŠ¸ì—…
    web_data: str  # ì›¹ í¬ë¡¤ë§ ë°ì´í„°
    retrieved_docs: List[Document]  # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤
    tech_evaluations: List[Dict]  # ê¸°ìˆ  í‰ê°€ ê²°ê³¼ë“¤ - Annotated ì œê±°í•˜ê³  ì¼ë°˜ Listë¡œ ë³€ê²½
    processing_index: int  # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ì¸ë±ìŠ¤
    vectorstore_ready: bool  # VectorStore ì¤€ë¹„ ì™„ë£Œ ì—¬ë¶€
    #

class Tech_functions():
    def load_pdf_documents(pdf_dir: Path) -> List[Document]:
        """PDF ë¬¸ì„œë“¤ì„ ë¡œë“œí•˜ê³  ì²­í‚¹"""
        all_documents = []
        
        # PDF íŒŒì¼ë“¤ ì°¾ê¸°
        pdf_files = list(pdf_dir.glob("*.pdf"))
        print(f"ë°œê²¬ëœ PDF íŒŒì¼: {len(pdf_files)}ê°œ")
        
        for pdf_file in pdf_files:
            try:
                print(f"ë¡œë”© ì¤‘: {pdf_file.name}")
                loader = PyPDFLoader(str(pdf_file))
                documents = loader.load()
                
                # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                for doc in documents:
                    doc.metadata["source_file"] = pdf_file.name
                    doc.metadata["source_type"] = "pdf"
                
                all_documents.extend(documents)
            except Exception as e:
                print(f"PDF ë¡œë“œ ì‹¤íŒ¨ ({pdf_file.name}): {e}")
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
        
        split_documents = text_splitter.split_documents(all_documents)
        print(f"ì´ {len(split_documents)}ê°œì˜ ì²­í¬ ìƒì„±")
        
        return split_documents
    

    def crawl_startup_info(startup_name: str, max_results: int = 5) -> str:
        """
        ìŠ¤íƒ€íŠ¸ì—… ì •ë³´ë¥¼ ì›¹ì—ì„œ í¬ë¡¤ë§ (Tavily)
        
        Args:
            startup_name: ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„
            max_results: ìˆ˜ì§‘í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
        
        Returns:
            í¬ë¡¤ë§ëœ í…ìŠ¤íŠ¸ ë°ì´í„°
        """
        print(f"\n{'='*60}")
        print(f"ì›¹ ê²€ìƒ‰ ì‹œì‘: {startup_name}")
        print(f"{'='*60}")
        
        # 1ìˆœìœ„: Tavily ì‹œë„
        try:
            result = crawl_with_tavily(startup_name, max_results)
            if result and len(result) > 100:  # ìœ ì˜ë¯¸í•œ ê²°ê³¼
                print(f"âœ“ Tavily ê²€ìƒ‰ ì„±ê³µ\n")
                return result
            else:
                print(f"âœ— Tavily ê²°ê³¼ ë¶€ì¡±, ëŒ€ì²´ ë°©ë²• ì‹œë„...\n")
        except Exception as e:
            print(f"âœ— Tavily ì˜¤ë¥˜: {str(e)[:50]}")


    def crawl_with_tavily(startup_name: str, max_results: int) -> str:
        """
        Tavily APIë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰ (LLM ìµœì í™”)
        
        Args:
            startup_name: ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„
            max_results: ìˆ˜ì§‘í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
        
        Returns:
            ê²€ìƒ‰ëœ í…ìŠ¤íŠ¸ ë°ì´í„°
        """
        from tavily import TavilyClient
        
        # API í‚¤ í™•ì¸
        api_key = os.getenv("TAVILY_API_KEY")
        if not api_key:
            raise ValueError("TAVILY_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        print(f"  [Tavily] ê²€ìƒ‰ ì¤‘...")
        
        # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        client = TavilyClient(api_key=api_key)
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        queries = [
            f"{startup_name} ìŠ¤íƒ€íŠ¸ì—… ê¸°ìˆ  í˜ì‹ ",
            f"{startup_name} AI íˆ¬ì ë¹„ì¦ˆë‹ˆìŠ¤"
        ]
        
        collected_text = []
        
        for idx, query in enumerate(queries, 1):
            try:
                print(f"    ì¿¼ë¦¬ {idx}: {query}")
                
                response = client.search(
                    query=query,
                    search_depth="basic",  # "basic" ë˜ëŠ” "advanced"
                    max_results=max_results,
                    include_answer=True,  # AI ìƒì„± ìš”ì•½ í¬í•¨
                    include_raw_content=False,
                    include_domains=None,  # ëª¨ë“  ë„ë©”ì¸ í—ˆìš©
                    days=365  # ìµœê·¼ 1ë…„ ë°ì´í„°
                )
                
                # AI ìƒì„± ìš”ì•½ (ë§¤ìš° ì¤‘ìš”!)
                if response.get('answer'):
                    collected_text.append(
                        f"[AI ìš”ì•½] {response['answer']}"
                    )
                    print(f"      âœ“ AI ìš”ì•½ ìˆ˜ì§‘")
                
                # ê²€ìƒ‰ ê²°ê³¼
                results = response.get('results', [])
                print(f"      âœ“ {len(results)}ê°œ ì¶œì²˜ ë°œê²¬")
                
                for result in results:
                    title = result.get('title', '')
                    content = result.get('content', '')
                    url = result.get('url', '')
                    score = result.get('score', 0)
                    
                    if content and len(content) > 50:
                        collected_text.append(
                            f"[ì¶œì²˜: {url}]\n"
                            f"ì œëª©: {title}\n"
                            f"ë‚´ìš©: {content}\n"
                            f"ê´€ë ¨ì„±: {score:.2f}"
                        )
                
                time.sleep(0.3)  # Rate limiting
                
            except Exception as e:
                print(f"      âœ— ì¿¼ë¦¬ ì‹¤íŒ¨: {str(e)[:50]}")
                continue
        
        if not collected_text:
            raise ValueError("Tavilyì—ì„œ ìœ ì˜ë¯¸í•œ ê²°ê³¼ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
        result_text = "\n\n".join(collected_text)
        print(f"    ì´ {len(collected_text)}ê°œ í•­ëª© ìˆ˜ì§‘")
        
        return result_text
    
    def initialize_vectorstore():
        """ChromaDB ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ë° EnsembleRetriever êµ¬ì„±"""
        import os.path
        import shutil

        # ====== ì„¤ì • ì˜µì…˜ ======
        FORCE_REBUILD = False  # Trueë¡œ ì„¤ì •í•˜ë©´ ê¸°ì¡´ DB ì‚­ì œí•˜ê³  ì¬ìƒì„±
        # =======================

        CHROMA_PERSIST_DIR = "../rag/tech/chroma_db"
        CHROMA_COLLECTION_NAME = "startup_tech_db"

        # ê°•ì œ ì¬ìƒì„± ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°
        if FORCE_REBUILD and os.path.exists(CHROMA_PERSIST_DIR):
            print("âš ï¸ FORCE_REBUILD=True: ê¸°ì¡´ VectorStoreë¥¼ ì‚­ì œí•©ë‹ˆë‹¤...")
            shutil.rmtree(CHROMA_PERSIST_DIR)
            print("âœ“ ì‚­ì œ ì™„ë£Œ\n")

        # ì´ë¯¸ ChromaDBê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if os.path.exists(CHROMA_PERSIST_DIR) and os.path.isdir(CHROMA_PERSIST_DIR):
            print("=" * 60)
            print("ğŸ“‚ ê¸°ì¡´ VectorStore ë°œê²¬!")
            print("=" * 60)
            print("ì €ì¥ëœ ì„ë² ë”© ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤ (ì„ë² ë”© ìƒì„± ìƒëµ)...\n")
            
            # ê¸°ì¡´ ChromaDB ë¡œë“œ (PDF ë¡œë“œ ë° ì„ë² ë”© ìƒì„± ìƒëµ)
            vectorstore = Chroma(
                collection_name=CHROMA_COLLECTION_NAME,
                embedding_function=embeddings,
                persist_directory=CHROMA_PERSIST_DIR
            )
            
            print(f"âœ“ VectorStore ë¡œë“œ ì™„ë£Œ!")
            
            # PDF ë¬¸ì„œë„ ë¡œë“œ (BM25ìš©ìœ¼ë¡œ í•„ìš”)
            print("\nPDF ë¬¸ì„œ ë¡œë“œ ì¤‘ (BM25 ì¸ë±ìŠ¤ìš©)...")
            pdf_documents = load_pdf_documents(PDF_DATA_PATH)
            
        else:
            print("=" * 60)
            print("ğŸ†• ê¸°ì¡´ VectorStore ì—†ìŒ - ìƒˆë¡œ ìƒì„±")
            print("=" * 60)
            print("ì„ë² ë”© ìƒì„± ì¤‘ (ì²˜ìŒ ì‹¤í–‰ ì‹œ ì‹œê°„ ì†Œìš”)...\n")
            
            # PDF ë¬¸ì„œ ë¡œë“œ
            print("PDF ë¬¸ì„œ ë¡œë“œ ì¤‘...")
            pdf_documents = load_pdf_documents(PDF_DATA_PATH)
            
            # ChromaDB ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            print("\nVectorStore ìƒì„± ì¤‘ (ì„ë² ë”© ìƒì„± - ìˆ˜ ë¶„ ì†Œìš” ê°€ëŠ¥)...")
            vectorstore = Chroma.from_documents(
                documents=pdf_documents,
                embedding=embeddings,
                collection_name=CHROMA_COLLECTION_NAME,
                persist_directory=CHROMA_PERSIST_DIR
            )
            
            print("âœ“ VectorStore ìƒì„± ì™„ë£Œ!")

        # ========== EnsembleRetriever êµ¬ì„± ==========
        print("\n" + "=" * 60)
        print("ğŸ”§ EnsembleRetriever êµ¬ì„± ì¤‘...")
        print("=" * 60)

        # 1. BM25Retriever ìƒì„± (í‚¤ì›Œë“œ ê¸°ë°˜)
        bm25_retriever = BM25Retriever.from_documents(pdf_documents)
        bm25_retriever.k = 5  # ìƒìœ„ 5ê°œ ë¬¸ì„œ ë°˜í™˜

        print(f"âœ“ BM25Retriever ìƒì„± ì™„ë£Œ (k={bm25_retriever.k})")

        # 2. Semantic Retriever ìƒì„± (ë²¡í„° ê¸°ë°˜)
        semantic_retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 5}
        )

        print(f"âœ“ SemanticRetriever ìƒì„± ì™„ë£Œ (k=5)")

        # 3. EnsembleRetrieverë¡œ ê²°í•©
        ensemble_retriever = EnsembleRetriever(
            retrievers=[bm25_retriever, semantic_retriever],
            weights=[0.5, 0.5]  # ë™ì¼í•œ ê°€ì¤‘ì¹˜
        )

        print(f"âœ“ EnsembleRetriever ìƒì„± ì™„ë£Œ (weights=[0.5, 0.5])")

        print(f"\n{'='*60}")
        print(f"âœ… ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"{'='*60}")
        print(f"PDF ë¬¸ì„œ ìˆ˜: {len(pdf_documents)}ê°œ")
        print(f"ì»¬ë ‰ì…˜ ì´ë¦„: {CHROMA_COLLECTION_NAME}")
        print(f"ì €ì¥ ìœ„ì¹˜: {CHROMA_PERSIST_DIR}")
        print(f"Retriever êµ¬ì„±: BM25 (50%) + Semantic (50%)")
        print(f"{'='*60}\n")

    def select_next_startup(state: TechState) -> TechState:
        """ë‹¤ìŒ í‰ê°€í•  ìŠ¤íƒ€íŠ¸ì—… ì„ íƒ"""
        idx = state.get("processing_index", 0)
        
        if idx < len(state["startup_names"]):
            state["current_startup"] = state["startup_names"][idx]
            # processing_indexëŠ” ì—¬ê¸°ì„œ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ (evaluateì—ì„œ ì—…ë°ì´íŠ¸)
            print(f"\n{'='*60}")
            print(f"[{idx+1}/{len(state['startup_names'])}] {state['current_startup']} í‰ê°€ ì‹œì‘")
            print(f"{'='*60}")
        
        return state


    def crawl_web_data(state: TechState) -> TechState:
        """ì›¹ì—ì„œ ìŠ¤íƒ€íŠ¸ì—… ì •ë³´ í¬ë¡¤ë§"""
        startup_name = state["current_startup"]
        
        # ì›¹ í¬ë¡¤ë§
        web_data = crawl_startup_info(startup_name, max_results=5)
        state["web_data"] = web_data
        
        return state


    def retrieve_tech_info(state: TechState) -> TechState:
        """PDF ë¬¸ì„œì—ì„œ ê´€ë ¨ ê¸°ìˆ  ì •ë³´ ê²€ìƒ‰"""
        startup_name = state["current_startup"]
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„± (ìŠ¤íƒ€íŠ¸ì—… íŠ¹ì„± ê³ ë ¤)
        query = f"{startup_name} AI ê¸°ìˆ  í˜ì‹  ìŠ¤íƒ€íŠ¸ì—… íˆ¬ì í‰ê°€ ê²½ìŸë ¥"
        
        print(f"\nPDF ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ ì¤‘...")
        # EnsembleRetrieverë¡œ ê²€ìƒ‰ ìˆ˜í–‰
        retrieved_docs = ensemble_retriever.get_relevant_documents(query)
        
        state["retrieved_docs"] = retrieved_docs
        print(f"ê²€ìƒ‰ ì™„ë£Œ: {len(retrieved_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")
        
        return state


    def evaluate_technology(state: TechState) -> TechState:
        """ì›¹ ë°ì´í„°ì™€ PDF ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¸°ìˆ ë ¥ í‰ê°€"""
        startup_name = state["current_startup"]
        web_data = state.get("web_data", "ì •ë³´ ì—†ìŒ")
        docs = state["retrieved_docs"]
        current_index = state.get("processing_index", 0)
        
        # ì´ë¯¸ í‰ê°€ëœ ê¸°ì—…ë“¤ì˜ ì ìˆ˜ í™•ì¸
        existing_evaluations = state.get("tech_evaluations", [])
        existing_scores = [e['ê¸°ìˆ _ì ìˆ˜'] for e in existing_evaluations if isinstance(e, dict)]
        
        # PDF ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        pdf_context = "\n\n".join([doc.page_content for doc in docs[:3]])  # ìƒìœ„ 3ê°œ
        
        print(f"\nGPT-4o-minië¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ìˆ ë ¥ í‰ê°€ ì¤‘...")
        print(f"  í˜„ì¬ê¹Œì§€ í‰ê°€ ì™„ë£Œ: {len(existing_evaluations)}ê°œ")
        
        # ê¸°ì¡´ ì ìˆ˜ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— ê°•í™”
        existing_scores_constraint = ""
        if existing_scores:
            scores_str = ", ".join(str(s) for s in existing_scores)
            existing_scores_constraint = f"""
    ### âš ï¸ ì¤‘ìš”í•œ ì œì•½ ì¡°ê±´ âš ï¸
    ì´ë¯¸ í‰ê°€í•œ ê¸°ì—…ë“¤ì˜ ì ìˆ˜: [{scores_str}]

    **í•„ìˆ˜**: ìƒˆë¡œìš´ ê¸°ìˆ _ì ìˆ˜ëŠ” ìœ„ ì ìˆ˜ë“¤ê³¼ **ìµœì†Œ 5ì  ì´ìƒ ì°¨ì´**ê°€ ë‚˜ì•¼ í•©ë‹ˆë‹¤.
    - ì´ë¯¸ ì‚¬ìš©ëœ ì ìˆ˜: {existing_scores}
    - ì‚¬ìš© ê¸ˆì§€ ë²”ìœ„: {', '.join(f'{s}Â±4ì ' for s in existing_scores)}
    - ê° ê¸°ì—…ì˜ ì‹¤ì œ ê°•ì ê³¼ ì•½ì ì„ ë°˜ì˜í•˜ì—¬ ì°¨ë³„í™”ëœ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ì„¸ìš”.
    """
        
        # í‰ê°€ í”„ë¡¬í”„íŠ¸ (ë‹¨ê³„ë³„ ì„¸ë¶€ í‰ê°€ ìš”êµ¬)
        eval_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ AI ìŠ¤íƒ€íŠ¸ì—… íˆ¬ì ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
    ì£¼ì–´ì§„ ì›¹ ì •ë³´ì™€ ì—…ê³„ íŠ¸ë Œë“œ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ìŠ¤íƒ€íŠ¸ì—…ì˜ ê¸°ìˆ ë ¥ì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.

    ## í‰ê°€ ê¸°ì¤€ (ë‹¨ê³„ë³„ í‰ê°€):

    **1ë‹¨ê³„: ê° í•­ëª©ë³„ ì ìˆ˜ ì‚°ì •**
    - ê¸°ìˆ ì˜ í˜ì‹ ì„± (0-30ì ): AI ê¸°ìˆ ì˜ ë…ì°½ì„±, ì°¨ë³„í™”ëœ ì ‘ê·¼ ë°©ì‹
    - ê¸°ìˆ ì˜ ì™„ì„±ë„ (0-30ì ): ì œí’ˆ/ì„œë¹„ìŠ¤ì˜ ì™„ì„±ë„, ì‹¤ì œ ì ìš© ì‚¬ë¡€
    - ì‹œì¥ ê²½ìŸë ¥ (0-20ì ): ê²½ìŸì‚¬ ëŒ€ë¹„ ìš°ìœ„, ì‹œì¥ í¬ì§€ì…”ë‹
    - íŠ¹í—ˆ/ì§€ì‹ì¬ì‚°ê¶Œ (0-10ì ): íŠ¹í—ˆ, ë…¼ë¬¸, ê¸°ìˆ  ìì‚°
    - ê¸°ìˆ  í™•ì¥ ê°€ëŠ¥ì„± (0-10ì ): ìŠ¤ì¼€ì¼ì—… ê°€ëŠ¥ì„±, ë‹¤ë¥¸ ë¶„ì•¼ ì ìš©

    **2ë‹¨ê³„: ì´ì  ê³„ì‚°**
    ìœ„ 5ê°œ í•­ëª©ì˜ ì ìˆ˜ë¥¼ í•©ì‚°í•˜ì—¬ ìµœì¢… ê¸°ìˆ _ì ìˆ˜ë¥¼ ë„ì¶œí•˜ì„¸ìš”.

    **ì¤‘ìš”**: 
    - ê¸°ì—…ë§ˆë‹¤ ëª…í™•íˆ ì°¨ë³„í™”ëœ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ì„¸ìš”
    - ëª¨ë“  ê¸°ì—…ì—ê²Œ ë¹„ìŠ·í•œ ì ìˆ˜ë¥¼ ì£¼ì§€ ë§ˆì„¸ìš”
    - ê° ê¸°ì—…ì˜ ì‹¤ì œ ê°•ì ê³¼ ì•½ì ì„ ì •í™•íˆ ë°˜ì˜í•˜ì„¸ìš”"""),
            ("user", """ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„: {startup_name}

    === ì›¹ì—ì„œ ìˆ˜ì§‘í•œ ì •ë³´ ===
    {web_data}

    === ì—…ê³„ íŠ¸ë Œë“œ ë° ì°¸ê³  ë¬¸ì„œ ===
    {pdf_context}

    {existing_scores_constraint}

    ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ë‹¨ê³„ë³„ë¡œ í‰ê°€**í•˜ê³  ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì‘ì„±í•˜ì„¸ìš”:

    {{
        "startup_name": "ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„",
        "í•­ëª©ë³„_ì ìˆ˜": {{
            "í˜ì‹ ì„±": ì ìˆ˜ (0-30),
            "ì™„ì„±ë„": ì ìˆ˜ (0-30),
            "ê²½ìŸë ¥": ì ìˆ˜ (0-20),
            "íŠ¹í—ˆ": ì ìˆ˜ (0-10),
            "í™•ì¥ì„±": ì ìˆ˜ (0-10)
        }},
        "ê¸°ìˆ _ì ìˆ˜": ì´ì  (0-100, ì •ìˆ˜),
        "ê¸°ìˆ _ë¶„ì„_ê·¼ê±°": "ê° í•­ëª©ë³„ ì ìˆ˜ ì‚°ì • ì´ìœ ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…. í˜ì‹ ì„±, ì™„ì„±ë„, ê²½ìŸë ¥, íŠ¹í—ˆ, í™•ì¥ì„± ê°ê°ì— ëŒ€í•´ ì›¹ ì •ë³´ë¥¼ ì¸ìš©í•˜ì—¬ ìƒì„¸íˆ ë¶„ì„"
    }}

    **í•„ìˆ˜**: ê¸°ìˆ _ì ìˆ˜ëŠ” í•­ëª©ë³„_ì ìˆ˜ì˜ í•©ê³¼ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.""")
        ])
        
        # LLM í˜¸ì¶œ
        chain = eval_prompt | llm
        response = chain.invoke({
            "startup_name": startup_name,
            "web_data": web_data[:2000],  # í† í° ì œí•œ ê³ ë ¤
            "pdf_context": pdf_context[:3000],
            "existing_scores_constraint": existing_scores_constraint
        })
        
        # JSON íŒŒì‹±
        try:
            content = response.content
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]
            
            evaluation = json.loads(content.strip())
            
            # í•­ëª©ë³„ ì ìˆ˜ê°€ ìˆìœ¼ë©´ ì´ì  ê²€ì¦
            if "í•­ëª©ë³„_ì ìˆ˜" in evaluation:
                item_scores = evaluation["í•­ëª©ë³„_ì ìˆ˜"]
                calculated_total = sum(item_scores.values())
                reported_total = evaluation.get("ê¸°ìˆ _ì ìˆ˜", calculated_total)
                
                # ì´ì ì´ ë§ì§€ ì•Šìœ¼ë©´ ì¬ê³„ì‚°
                if abs(calculated_total - reported_total) > 1:
                    print(f"  âš ï¸ ì ìˆ˜ ë¶ˆì¼ì¹˜ ê°ì§€ (ë³´ê³ : {reported_total}, ê³„ì‚°: {calculated_total}) - ì¬ê³„ì‚°ëœ ê°’ ì‚¬ìš©")
                    evaluation["ê¸°ìˆ _ì ìˆ˜"] = calculated_total
            
            print(f"í‰ê°€ ì™„ë£Œ: {evaluation['ê¸°ìˆ _ì ìˆ˜']}ì ")
            
            # í•­ëª©ë³„ ì ìˆ˜ ì¶œë ¥
            if "í•­ëª©ë³„_ì ìˆ˜" in evaluation:
                scores_breakdown = ", ".join([f"{k}={v}" for k, v in evaluation["í•­ëª©ë³„_ì ìˆ˜"].items()])
                print(f"  ì„¸ë¶€: {scores_breakdown}")
            
        except Exception as e:
            print(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            print(f"ì‘ë‹µ ë‚´ìš©: {response.content[:300]}")
            evaluation = {
                "startup_name": startup_name,
                "ê¸°ìˆ _ì ìˆ˜": 50,  # ê¸°ë³¸ê°’
                "ê¸°ìˆ _ë¶„ì„_ê·¼ê±°": f"í‰ê°€ ì‹¤íŒ¨: {str(e)}"
            }
        
        # â˜… ì¤‘ìš”: ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸ì— append (ë®ì–´ì“°ê¸° ì•„ë‹˜!)
        current_evaluations = state.get("tech_evaluations", [])
        current_evaluations.append(evaluation)
        state["tech_evaluations"] = current_evaluations
        
        # processing_index ì¦ê°€ (ë‹¤ìŒ ê¸°ì—…ìœ¼ë¡œ)
        state["processing_index"] = current_index + 1
        
        print(f"ì§„í–‰ ìƒí™©: {state['processing_index']}/{len(state['startup_names'])} ì™„ë£Œ")
        print(f"  ëˆ„ì  í‰ê°€ ê²°ê³¼: {len(state['tech_evaluations'])}ê°œ\n")
        
        return state


    def check_completion(state: TechState) -> str:
        """ëª¨ë“  ìŠ¤íƒ€íŠ¸ì—… í‰ê°€ ì™„ë£Œ ì—¬ë¶€ í™•ì¸"""
        idx = state.get("processing_index", 0)
        total = len(state.get("startup_names", []))
        
        print(f"[check_completion] í˜„ì¬ ì¸ë±ìŠ¤: {idx}, ì „ì²´: {total}")
        
        if idx < total:
            return "continue"  # ë‹¤ìŒ ìŠ¤íƒ€íŠ¸ì—… í‰ê°€
        else:
            return "end"  # ëª¨ë“  í‰ê°€ ì™„ë£Œ

# StateGraph ìƒì„±
tech_workflow = StateGraph(TechState)
    
# ë…¸ë“œ ì¶”ê°€
tech_workflow.add_node("select_startup", select_next_startup)
tech_workflow.add_node("crawl_web", crawl_web_data)
tech_workflow.add_node("retrieve_info", retrieve_tech_info)
tech_workflow.add_node("evaluate", evaluate_technology)

# ì—£ì§€ ì„¤ì •
tech_workflow.set_entry_point("select_startup")
tech_workflow.add_edge("select_startup", "crawl_web")
tech_workflow.add_edge("crawl_web", "retrieve_info")
tech_workflow.add_edge("retrieve_info", "evaluate")

# ì¡°ê±´ë¶€ ì—£ì§€ (í‰ê°€ ì™„ë£Œ í›„ ë‹¤ìŒ ìŠ¤íƒ€íŠ¸ì—…ìœ¼ë¡œ ì´ë™ ë˜ëŠ” ì¢…ë£Œ)
tech_workflow.add_conditional_edges(
    "evaluate",
    check_completion,
    {
        "continue": "select_startup",
        "end": END
    }
)

# ê·¸ë˜í”„ ì»´íŒŒì¼
app = tech_workflow.compile()

print("\nì›Œí¬í”Œë¡œìš° êµ¬ì„± ì™„ë£Œ!")
print("ìˆœì„œ: select_startup -> crawl_web -> retrieve_info -> evaluate -> [ë°˜ë³µ or ì¢…ë£Œ]")

# í‰ê°€í•  ìŠ¤íƒ€íŠ¸ì—… ë¦¬ìŠ¤íŠ¸
startups_to_evaluate = [
    "ë¦¬ë¸Œ ì• ë‹ˆì›¨ì–´",
    "ì–´ë”©",
    "íŠ¸ë¦½ë¹„í† ì¦ˆ",
    "íŠ¸ë¦½ì†Œë‹¤",
    "í•˜ì´ì–´í”Œë ˆì´ìŠ¤"
]

# ì´ˆê¸° ìƒíƒœ ì„¤ì • (ëª…í™•í•œ ì´ˆê¸°í™”)
initial_state = {
    "startup_names": startups_to_evaluate,
    "current_startup": "",
    "web_data": "",
    "retrieved_docs": [],
    "tech_evaluations": [],  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ëª…í™•íˆ ì´ˆê¸°í™”
    "processing_index": 0,
    "vectorstore_ready": True
}

# ì—ì´ì „íŠ¸ ì‹¤í–‰
print("\n" + "=" * 60)
print("AI ìŠ¤íƒ€íŠ¸ì—… ê¸°ìˆ  í‰ê°€ ì—ì´ì „íŠ¸ ì‹œì‘")
print(f"í‰ê°€ ëŒ€ìƒ: {len(startups_to_evaluate)}ê°œ ê¸°ì—…")
print("=" * 60)

# ê·¸ë˜í”„ ì¬ì»´íŒŒì¼ (ì´ì „ ìƒíƒœ ì´ˆê¸°í™”)
app = workflow.compile()

result = app.invoke(initial_state)

print("\n" + "=" * 60)
print("ì „ì²´ í‰ê°€ ì™„ë£Œ")
print(f"ìµœì¢… í‰ê°€ ê²°ê³¼ ìˆ˜: {len(result['tech_evaluations'])}ê°œ")
print("=" * 60)


# í‰ê°€ ê²°ê³¼ ì¶œë ¥
evaluations = result["tech_evaluations"]

print(f"\n\n{'#'*60}")
print(f"ì´ {len(evaluations)}ê°œ ìŠ¤íƒ€íŠ¸ì—… í‰ê°€ ì™„ë£Œ")
print(f"{'#'*60}\n")

for i, eval_data in enumerate(evaluations, 1):
    print(f"\n{'='*60}")
    print(f"[{i}] {eval_data['startup_name']}")
    print(f"{'='*60}")
    print(f"\nê¸°ìˆ  ì ìˆ˜: {eval_data['ê¸°ìˆ _ì ìˆ˜']}/100ì ")
    print(f"\në¶„ì„ ê·¼ê±°:")
    print(f"{eval_data['ê¸°ìˆ _ë¶„ì„_ê·¼ê±°']}")
    print()