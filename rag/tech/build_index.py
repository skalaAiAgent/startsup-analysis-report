"""
TechAgentìš© PDF ì¸ë±ì„œ
data/ í´ë”ì˜ íŠ¹ì • PDF íŒŒì¼ë“¤ì„ ChromaDBì— ì¸ë±ì‹±í•©ë‹ˆë‹¤.
"""

import os
import sys
from pathlib import Path
from typing import List

from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from dotenv import load_dotenv

load_dotenv()


def load_pdf_documents(pdf_files: List[str]) -> List[Document]:
    """
    ì§€ì •ëœ PDF ë¬¸ì„œë“¤ì„ ë¡œë“œí•˜ê³  ì²­í‚¹
    
    Args:
        pdf_files: PDF íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        List[Document]: ì²­í¬ë¡œ ë¶„í• ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    print(f"\n{'='*60}")
    print(f"PDF íŒŒì¼ ë¡œë”©")
    print(f"{'='*60}")
    print(f"ğŸ“„ ëŒ€ìƒ íŒŒì¼: {len(pdf_files)}ê°œ\n")
    
    all_documents = []
    loaded_files = []
    
    for idx, pdf_file in enumerate(pdf_files, 1):
        pdf_path = Path(pdf_file)
        
        if not pdf_path.exists():
            print(f"  [{idx}/{len(pdf_files)}] âš ï¸  íŒŒì¼ ì—†ìŒ: {pdf_path.name}")
            print(f"      ê²½ë¡œ: {pdf_file}")
            continue
        
        try:
            print(f"  [{idx}/{len(pdf_files)}] ë¡œë”© ì¤‘: {pdf_path.name}")
            loader = PyPDFLoader(str(pdf_path))
            documents = loader.load()
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for doc in documents:
                doc.metadata["source_file"] = pdf_path.name
                doc.metadata["source_type"] = "pdf"
            
            all_documents.extend(documents)
            loaded_files.append(pdf_path.name)
            print(f"      âœ“ {len(documents)}í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"      âœ— PDF ë¡œë“œ ì‹¤íŒ¨: {e}")
            continue
    
    if not all_documents:
        raise ValueError(f"ë¡œë“œëœ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    
    print(f"\nâœ… ì´ {len(all_documents)}í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")
    print(f"   ë¡œë“œëœ íŒŒì¼: {', '.join(loaded_files)}")
    
    # í…ìŠ¤íŠ¸ ì²­í‚¹
    print(f"\n{'='*60}")
    print(f"í…ìŠ¤íŠ¸ ì²­í‚¹")
    print(f"{'='*60}")
    print(f"ì²­í¬ í¬ê¸°: 1000ì")
    print(f"ì¤‘ë³µ í¬ê¸°: 200ì\n")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
    )
    
    split_documents = text_splitter.split_documents(all_documents)
    print(f"âœ“ ì´ {len(split_documents)}ê°œì˜ ì²­í¬ ìƒì„± ì™„ë£Œ\n")
    
    return split_documents


def build_tech_index(
    pdf_files: List[str] = None,
    chroma_persist_dir: str = None,
    collection_name: str = "startup_tech_db",
    force_rebuild: bool = False
) -> None:
    """
    ì§€ì •ëœ PDF íŒŒì¼ë“¤ì„ ChromaDBì— ì¸ë±ì‹±
    
    Args:
        pdf_files: PDF íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸. Noneì´ë©´ ê¸°ë³¸ íŒŒì¼ë“¤ ì‚¬ìš©
        chroma_persist_dir: ChromaDB ì €ì¥ ê²½ë¡œ. Noneì´ë©´ rag/tech ì‚¬ìš©
        collection_name: ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„ (ê¸°ë³¸: startup_tech_db)
        force_rebuild: Trueì¼ ê²½ìš° ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ í›„ ì¬ìƒì„±
    """
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ê³„ì‚°
    root = os.path.normpath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir))
    data_dir = os.path.join(root, "data")
    
    # ê¸°ë³¸ PDF íŒŒì¼ ëª©ë¡
    if pdf_files is None:
        pdf_files = [
            os.path.join(data_dir, "ê¸°ìˆ ìš”ì•½_ì „ì²´_ê¸°ì—…_ì¸í„°ë·°.pdf"),
            os.path.join(data_dir, "ì‹œì¥ì„±ë¶„ì„_ìŠ¤íƒ€íŠ¸ì—…_ì‹œì¥ì „ëµ_ë°_ìƒíƒœê³„.pdf"),
            os.path.join(data_dir, "ê¸°ì—…ë¹„êµ.pdf")
        ]
    
    # ChromaDB ê²½ë¡œ ì„¤ì •
    if chroma_persist_dir is None:
        chroma_persist_dir = os.path.join(root, "rag", "tech")
    
    print(f"\n{'='*60}")
    print(f"TechAgentìš© PDF ì¸ë±ì„œ")
    print(f"{'='*60}")
    print(f"í”„ë¡œì íŠ¸ ë£¨íŠ¸: {root}")
    print(f"ë°ì´í„° ë””ë ‰í† ë¦¬: {data_dir}")
    print(f"\nëŒ€ìƒ íŒŒì¼:")
    for pdf_file in pdf_files:
        exists = "âœ“" if os.path.exists(pdf_file) else "âœ—"
        print(f"  {exists} {os.path.basename(pdf_file)}")
    print(f"\nChromaDB ê²½ë¡œ: {chroma_persist_dir}")
    print(f"ì»¬ë ‰ì…˜ ì´ë¦„: {collection_name}")
    print(f"ê°•ì œ ì¬ìƒì„±: {force_rebuild}")
    print(f"{'='*60}\n")
    
    # ê¸°ì¡´ ì¸ë±ìŠ¤ í™•ì¸
    if os.path.exists(chroma_persist_dir) and os.path.isdir(chroma_persist_dir):
        if force_rebuild:
            print("ğŸ—‘ï¸  ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ ì¤‘...")
            import shutil
            shutil.rmtree(chroma_persist_dir)
            print("âœ“ ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ ì™„ë£Œ\n")
        else:
            print("âš ï¸  ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•©ë‹ˆë‹¤!")
            print(f"   ê²½ë¡œ: {chroma_persist_dir}")
            print(f"   ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ì‚­ì œí•˜ê³  ì¬ìƒì„±í•˜ë ¤ë©´ force_rebuild=Trueë¡œ ì„¤ì •í•˜ì„¸ìš”.")
            print(f"   ì˜ˆ: build_tech_index(force_rebuild=True)\n")
            
            response = input("ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ì‚­ì œí•˜ê³  ì¬ìƒì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
            if response.lower() != 'y':
                print("\nâŒ ì¸ë±ì‹± ì·¨ì†Œë¨")
                return
            
            print("\nğŸ—‘ï¸  ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ ì¤‘...")
            import shutil
            shutil.rmtree(chroma_persist_dir)
            print("âœ“ ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ ì™„ë£Œ\n")
    
    try:
        # 1. PDF ë¬¸ì„œ ë¡œë“œ ë° ì²­í‚¹
        documents = load_pdf_documents(pdf_files)
        
        # 2. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        print(f"{'='*60}")
        print(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”")
        print(f"{'='*60}")
        print(f"ëª¨ë¸: nomic-embed-text (Ollama)\n")
        
        embeddings = OllamaEmbeddings(model="nomic-embed-text")
        print("âœ“ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ\n")
        
        # 3. ChromaDBì— ì¸ë±ì‹±
        print(f"{'='*60}")
        print(f"ChromaDB ì¸ë±ì‹± ì¤‘...")
        print(f"{'='*60}")
        print(f"âš ï¸  ì„ë² ë”© ìƒì„± ì¤‘... (ìˆ˜ ë¶„ ì†Œìš” ê°€ëŠ¥)\n")
        
        vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=embeddings,
            collection_name=collection_name,
            persist_directory=chroma_persist_dir
        )
        
        print(f"\n{'='*60}")
        print(f"âœ… ì¸ë±ì‹± ì™„ë£Œ!")
        print(f"{'='*60}")
        print(f"ì €ì¥ ê²½ë¡œ: {chroma_persist_dir}")
        print(f"ì»¬ë ‰ì…˜: {collection_name}")
        print(f"ì´ ì²­í¬ ìˆ˜: {len(documents)}ê°œ")
        print(f"{'='*60}\n")
        
        # 4. ì¸ë±ìŠ¤ í…ŒìŠ¤íŠ¸
        print(f"{'='*60}")
        print(f"ì¸ë±ìŠ¤ í…ŒìŠ¤íŠ¸")
        print(f"{'='*60}")
        test_query = "AI ìŠ¤íƒ€íŠ¸ì—… ê¸°ìˆ  í˜ì‹ "
        print(f"í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: '{test_query}'\n")
        
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 3}
        )
        test_docs = retriever.invoke(test_query)
        
        print(f"ê²€ìƒ‰ ê²°ê³¼: {len(test_docs)}ê°œ ë¬¸ì„œ")
        for idx, doc in enumerate(test_docs, 1):
            source = doc.metadata.get("source_file", "Unknown")
            content_preview = doc.page_content[:100].replace("\n", " ")
            print(f"  [{idx}] {source}")
            print(f"      {content_preview}...\n")
        
        print(f"{'='*60}")
        print(f"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì¸ë±ìŠ¤ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤!")
        print(f"{'='*60}\n")
        
    except Exception as e:
        print(f"\n{'='*60}")
        print(f"âŒ ì¸ë±ì‹± ì‹¤íŒ¨")
        print(f"{'='*60}")
        print(f"ì˜¤ë¥˜: {e}\n")
        import traceback
        traceback.print_exc()
        sys.exit(1)


def verify_index(
    chroma_persist_dir: str = None,
    collection_name: str = "startup_tech_db"
) -> None:
    """
    ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.
    
    Args:
        chroma_persist_dir: ChromaDB ì €ì¥ ê²½ë¡œ. Noneì´ë©´ rag/tech ì‚¬ìš©
        collection_name: ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„
    """
    # ChromaDB ê²½ë¡œ ì„¤ì •
    if chroma_persist_dir is None:
        root = os.path.normpath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir))
        chroma_persist_dir = os.path.join(root, "rag", "tech")
    
    print(f"\n{'='*60}")
    print(f"ì¸ë±ìŠ¤ ê²€ì¦")
    print(f"{'='*60}")
    print(f"ê²½ë¡œ: {chroma_persist_dir}")
    print(f"ì»¬ë ‰ì…˜: {collection_name}")
    print(f"{'='*60}\n")
    
    if not os.path.exists(chroma_persist_dir):
        print(f"âŒ ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {chroma_persist_dir}")
        print(f"   ë¨¼ì € build_tech_index()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\n")
        return
    
    try:
        embeddings = OllamaEmbeddings(model="nomic-embed-text")
        
        vectorstore = Chroma(
            collection_name=collection_name,
            embedding_function=embeddings,
            persist_directory=chroma_persist_dir
        )
        
        # ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸
        collection = vectorstore._collection
        count = collection.count()
        
        print(f"âœ“ ì¸ë±ìŠ¤ ë¡œë“œ ì„±ê³µ")
        print(f"  ì´ ì²­í¬ ìˆ˜: {count}ê°œ\n")
        
        # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
        test_queries = [
            "AI ìŠ¤íƒ€íŠ¸ì—… ê¸°ìˆ ",
            "íˆ¬ì í‰ê°€",
            "í˜ì‹  ê¸°ìˆ "
        ]
        
        print(f"{'='*60}")
        print(f"í…ŒìŠ¤íŠ¸ ê²€ìƒ‰")
        print(f"{'='*60}\n")
        
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 2}
        )
        
        for query in test_queries:
            print(f"ì¿¼ë¦¬: '{query}'")
            docs = retriever.invoke(query)
            print(f"ê²°ê³¼: {len(docs)}ê°œ ë¬¸ì„œ")
            
            if docs:
                doc = docs[0]
                source = doc.metadata.get("source_file", "Unknown")
                content_preview = doc.page_content[:80].replace("\n", " ")
                print(f"  ìƒìœ„ ê²°ê³¼: {source}")
                print(f"  ë‚´ìš©: {content_preview}...\n")
            else:
                print(f"  âš ï¸  ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ\n")
        
        print(f"{'='*60}")
        print(f"âœ… ê²€ì¦ ì™„ë£Œ - ì¸ë±ìŠ¤ê°€ ì •ìƒì…ë‹ˆë‹¤!")
        print(f"{'='*60}\n")
        
    except Exception as e:
        print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {e}\n")
        import traceback
        traceback.print_exc()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="TechAgentìš© PDF ì¸ë±ì„œ")
    parser.add_argument(
        "--pdf-files",
        type=str,
        nargs="+",
        default=None,
        help="ì¸ë±ì‹±í•  PDF íŒŒì¼ ê²½ë¡œë“¤ (ë¯¸ì§€ì •ì‹œ ê¸°ë³¸ 3ê°œ íŒŒì¼ ì‚¬ìš©)"
    )
    parser.add_argument(
        "--chroma-path",
        type=str,
        default=None,
        help="ChromaDB ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: {í”„ë¡œì íŠ¸}/rag/tech)"
    )
    parser.add_argument(
        "--collection",
        type=str,
        default="startup_tech_db",
        help="ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„ (ê¸°ë³¸: startup_tech_db)"
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ í›„ ì¬ìƒì„±"
    )
    parser.add_argument(
        "--verify",
        action="store_true",
        help="ê¸°ì¡´ ì¸ë±ìŠ¤ ê²€ì¦ë§Œ ìˆ˜í–‰"
    )
    
    args = parser.parse_args()
    
    if args.verify:
        # ê²€ì¦ë§Œ ìˆ˜í–‰
        verify_index(
            chroma_persist_dir=args.chroma_path,
            collection_name=args.collection
        )
    else:
        # ì¸ë±ì‹± ìˆ˜í–‰
        build_tech_index(
            pdf_files=args.pdf_files,
            chroma_persist_dir=args.chroma_path,
            collection_name=args.collection,
            force_rebuild=args.force
        )


if __name__ == "__main__":
    main()