{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "427a12b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TechAgent ì´ˆê¸°í™”\n",
      "============================================================\n",
      "í‰ê°€ ëŒ€ìƒ: ì–´ë”©\n",
      "PDF ê²½ë¡œ: ..\\data\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "VectorStore ì´ˆê¸°í™”\n",
      "============================================================\n",
      "\n",
      "ğŸ“‚ ê¸°ì¡´ VectorStore ë°œê²¬ - ë¡œë“œ ì¤‘...\n",
      "âœ“ VectorStore ë¡œë“œ ì™„ë£Œ\n",
      "\n",
      "PDF ë¬¸ì„œ ë¡œë“œ ì¤‘ (BM25 ì¸ë±ìŠ¤ìš©)...\n",
      "ë°œê²¬ëœ PDF íŒŒì¼: 3ê°œ\n",
      "  ë¡œë”© ì¤‘: ê¸°ìˆ ìš”ì•½_ì „ì²´_ê¸°ì—…_ì¸í„°ë·°.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ë¡œë”© ì¤‘: ê¸°ì—…ë¹„êµ.pdf\n",
      "  ë¡œë”© ì¤‘: ì‹œì¥ì„±ë¶„ì„_ìŠ¤íƒ€íŠ¸ì—…_ì‹œì¥ì „ëµ_ë°_ìƒíƒœê³„.pdf\n",
      "ì´ 134ê°œì˜ ì²­í¬ ìƒì„±\n",
      "\n",
      "============================================================\n",
      "EnsembleRetriever êµ¬ì„± ì¤‘...\n",
      "============================================================\n",
      "\n",
      "âœ“ BM25Retriever ìƒì„± ì™„ë£Œ (k=5)\n",
      "âœ“ SemanticRetriever ìƒì„± ì™„ë£Œ (k=5)\n",
      "âœ“ EnsembleRetriever ìƒì„± ì™„ë£Œ (weights=[0.5, 0.5])\n",
      "\n",
      "\n",
      "ì›Œí¬í”Œë¡œìš° êµ¬ì„± ì™„ë£Œ!\n",
      "ìˆœì„œ: select_startup -> crawl_web -> retrieve_info -> evaluate -> [ë°˜ë³µ or ì¢…ë£Œ]\n",
      "\n",
      "\n",
      "============================================================\n",
      "AI ìŠ¤íƒ€íŠ¸ì—… ê¸°ìˆ  í‰ê°€ ì—ì´ì „íŠ¸ ì‹œì‘\n",
      "í‰ê°€ ëŒ€ìƒ: 1ê°œ ê¸°ì—…\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "[1/1] ì–´ë”© í‰ê°€ ì‹œì‘\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ì›¹ ê²€ìƒ‰ ì‹œì‘: ì–´ë”©\n",
      "============================================================\n",
      "  [Tavily] ê²€ìƒ‰ ì¤‘...\n",
      "    ì¿¼ë¦¬ 1: ì–´ë”© ìŠ¤íƒ€íŠ¸ì—… ê¸°ìˆ  í˜ì‹ \n",
      "      âœ“ AI ìš”ì•½ ìˆ˜ì§‘\n",
      "      âœ“ 5ê°œ ì¶œì²˜ ë°œê²¬\n",
      "    ì¿¼ë¦¬ 2: ì–´ë”© AI íˆ¬ì ë¹„ì¦ˆë‹ˆìŠ¤\n",
      "      âœ“ AI ìš”ì•½ ìˆ˜ì§‘\n",
      "      âœ“ 5ê°œ ì¶œì²˜ ë°œê²¬\n",
      "    ì´ 12ê°œ í•­ëª© ìˆ˜ì§‘\n",
      "âœ“ Tavily ê²€ìƒ‰ ì„±ê³µ\n",
      "\n",
      "\n",
      "PDF ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ ì¤‘...\n",
      "ê²€ìƒ‰ ì™„ë£Œ: 8ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨\n",
      "\n",
      "GPT-4o-minië¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ìˆ ë ¥ í‰ê°€ ì¤‘...\n",
      "  í˜„ì¬ê¹Œì§€ í‰ê°€ ì™„ë£Œ: 0ê°œ\n",
      "í‰ê°€ ì™„ë£Œ: 84ì \n",
      "  ì„¸ë¶€: í˜ì‹ ì„±=25, ì™„ì„±ë„=28, ê²½ìŸë ¥=18, íŠ¹í—ˆ=5, í™•ì¥ì„±=8\n",
      "ì§„í–‰ ìƒí™©: 1/1 ì™„ë£Œ\n",
      "  ëˆ„ì  í‰ê°€ ê²°ê³¼: 1ê°œ\n",
      "\n",
      "\n",
      "============================================================\n",
      "ì „ì²´ í‰ê°€ ì™„ë£Œ\n",
      "ìµœì¢… í‰ê°€ ê²°ê³¼ ìˆ˜: 1ê°œ\n",
      "============================================================\n",
      "\n",
      "{'startup_name': 'ì–´ë”©', 'í•­ëª©ë³„_ì ìˆ˜': {'í˜ì‹ ì„±': 25, 'ì™„ì„±ë„': 28, 'ê²½ìŸë ¥': 18, 'íŠ¹í—ˆ': 5, 'í™•ì¥ì„±': 8}, 'ê¸°ìˆ _ì ìˆ˜': 84, 'ê¸°ìˆ _ë¶„ì„_ê·¼ê±°': 'ì–´ë”©ì€ ì—¬í–‰ì‚°ì—…ì˜ ë””ì§€í„¸ í˜ì‹ ì„ ì£¼ë„í•˜ëŠ” SaaS ê¸°ë°˜ í”Œë«í¼ì„ ì œê³µí•˜ì—¬ ì—¬í–‰ì‚¬ì˜ ë³µì¡í•œ ì—…ë¬´ë¥¼ ê°„ì†Œí™”í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ê¸°ìˆ ì˜ í˜ì‹ ì„± ì¸¡ë©´ì—ì„œ ë†’ì€ ì ìˆ˜ë¥¼ ë¶€ì—¬í•  ìˆ˜ ìˆëŠ” ìš”ì†Œì…ë‹ˆë‹¤. íŠ¹íˆ, ì—¬í–‰ì—…ê³„ì—ì„œì˜ ê¹Šì€ ì´í•´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ì ‘ê·¼ ë°©ì‹ì€ ì°¨ë³„í™”ëœ ì ìœ¼ë¡œ í‰ê°€ë©ë‹ˆë‹¤(í˜ì‹ ì„±: 25ì ). ì œí’ˆì˜ ì™„ì„±ë„ëŠ” 2022ë…„ ì‹œë¦¬ì¦ˆ A íˆ¬ìë¥¼ í†µí•´ 36ì–µì›ì„ ìœ ì¹˜í•˜ê³ , ë§¤ì¶œì´ ê¸‰ì¦í•œ ì‚¬ë¡€ì—ì„œ ë‚˜íƒ€ë‚˜ë“¯ì´ ì‹¤ì œ ì ìš© ì‚¬ë¡€ê°€ ë’·ë°›ì¹¨ë˜ê³  ìˆìŠµë‹ˆë‹¤(ì™„ì„±ë„: 28ì ). ê²½ìŸë ¥ ì¸¡ë©´ì—ì„œëŠ” ì¤‘ì†Œí˜• ì—¬í–‰ì‚¬ë“¤ì´ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í™˜ê²½ì„ ì œê³µí•¨ìœ¼ë¡œì¨ ì‹œì¥ì—ì„œì˜ í¬ì§€ì…”ë‹ì´ ê¸ì •ì ì´ë‚˜, ê²½ìŸì‚¬ì™€ì˜ ë¹„êµì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ ì•½ê°„ì˜ ìš°ìœ„ë¥¼ ë³´ì´ëŠ” ìˆ˜ì¤€ì…ë‹ˆë‹¤(ê²½ìŸë ¥: 18ì ). íŠ¹í—ˆë‚˜ ì§€ì‹ì¬ì‚°ê¶Œ ê´€ë ¨ ì •ë³´ëŠ” ë¶€ì¡±í•˜ì—¬ ë‚®ì€ ì ìˆ˜ë¥¼ ë¶€ì—¬í–ˆìŠµë‹ˆë‹¤(íŠ¹í—ˆ: 5ì ). ë§ˆì§€ë§‰ìœ¼ë¡œ, SaaS ëª¨ë¸ì€ ë‹¤ë¥¸ ì‚°ì—…ìœ¼ë¡œì˜ í™•ì¥ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì£¼ê³  ìˆìœ¼ë©°, ì´ëŠ” ê¸ì •ì ì¸ ìš”ì†Œë¡œ í‰ê°€ë˜ì–´ ì ìˆ˜ë¥¼ ë¶€ì—¬í–ˆìŠµë‹ˆë‹¤(í™•ì¥ì„±: 8ì ).'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AI ìŠ¤íƒ€íŠ¸ì—… ê¸°ìˆ  í‰ê°€ ì—ì´ì „íŠ¸\n",
    "Langgraphë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ í¬ë¡¤ë§ + PDF ë¶„ì„ì„ í†µí•œ ê¸°ìˆ ë ¥ í‰ê°€\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import TypedDict, List, Dict\n",
    "import json\n",
    "import time\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, END\n",
    "from dotenv import load_dotenv\n",
    "from tavily import TavilyClient\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class TechState(TypedDict):\n",
    "    \"\"\"ì—ì´ì „íŠ¸ì˜ ìƒíƒœë¥¼ ì •ì˜í•˜ëŠ” í´ë˜ìŠ¤\"\"\"\n",
    "    startup_names: List[str]\n",
    "    current_startup: str\n",
    "    web_data: str\n",
    "    retrieved_docs: List[Document]\n",
    "    tech_evaluations: List[Dict]\n",
    "    processing_index: int\n",
    "    vectorstore_ready: bool\n",
    "\n",
    "\n",
    "class TechAgent:\n",
    "    \"\"\"\n",
    "    AI ìŠ¤íƒ€íŠ¸ì—… ê¸°ìˆ  í‰ê°€ ì—ì´ì „íŠ¸\n",
    "\n",
    "    ì‚¬ìš©ë²•:\n",
    "        agent = TechAgent(startups_to_evaluate=\"ì–´ë”©\")\n",
    "        result = agent.get_tech_result()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, startups_to_evaluate: str | List[str], pdf_data_path: str = \"../data\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            startups_to_evaluate: í‰ê°€í•  ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„ (ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸)\n",
    "            pdf_data_path: PDF ë°ì´í„° ê²½ë¡œ (ê¸°ë³¸ê°’: í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ data í´ë”)\n",
    "        \"\"\"\n",
    "        # ìŠ¤íƒ€íŠ¸ì—… ë¦¬ìŠ¤íŠ¸ ì„¤ì •\n",
    "        if isinstance(startups_to_evaluate, str):\n",
    "            self.startup_names = [startups_to_evaluate]\n",
    "        else:\n",
    "            self.startup_names = startups_to_evaluate\n",
    "\n",
    "        # PDF ë°ì´í„° ê²½ë¡œ ì„¤ì •\n",
    "        if pdf_data_path is None:\n",
    "            # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ data í´ë” ì‚¬ìš©\n",
    "            self.pdf_data_path = Path(ROOT_DIR).parent / \"data\"\n",
    "        else:\n",
    "            self.pdf_data_path = Path(pdf_data_path)\n",
    "\n",
    "        # ëª¨ë¸ ì´ˆê¸°í™”\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
    "        self.embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "        # ChromaDB ê²½ë¡œ\n",
    "        self.chroma_persist_dir = \"../rag/tech\"\n",
    "        self.chroma_collection_name = \"startup_tech_db\"\n",
    "\n",
    "        # VectorStore ë° Retriever ì´ˆê¸°í™”\n",
    "        self.vectorstore = None\n",
    "        self.ensemble_retriever = None\n",
    "        self.pdf_documents = None\n",
    "\n",
    "        # Workflow ì´ˆê¸°í™”\n",
    "        self.app = None\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TechAgent ì´ˆê¸°í™”\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"í‰ê°€ ëŒ€ìƒ: {', '.join(self.startup_names)}\")\n",
    "        print(f\"PDF ê²½ë¡œ: {self.pdf_data_path}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "    def _load_pdf_documents(self) -> List[Document]:\n",
    "        \"\"\"PDF ë¬¸ì„œë“¤ì„ ë¡œë“œí•˜ê³  ì²­í‚¹\"\"\"\n",
    "        all_documents = []\n",
    "\n",
    "        pdf_files = list(self.pdf_data_path.glob(\"*.pdf\"))\n",
    "        print(f\"ë°œê²¬ëœ PDF íŒŒì¼: {len(pdf_files)}ê°œ\")\n",
    "\n",
    "        for pdf_file in pdf_files:\n",
    "            try:\n",
    "                print(f\"  ë¡œë”© ì¤‘: {pdf_file.name}\")\n",
    "                loader = PyPDFLoader(str(pdf_file))\n",
    "                documents = loader.load()\n",
    "\n",
    "                for doc in documents:\n",
    "                    doc.metadata[\"source_file\"] = pdf_file.name\n",
    "                    doc.metadata[\"source_type\"] = \"pdf\"\n",
    "\n",
    "                all_documents.extend(documents)\n",
    "            except Exception as e:\n",
    "                print(f\"  PDF ë¡œë“œ ì‹¤íŒ¨ ({pdf_file.name}): {e}\")\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "        )\n",
    "\n",
    "        split_documents = text_splitter.split_documents(all_documents)\n",
    "        print(f\"ì´ {len(split_documents)}ê°œì˜ ì²­í¬ ìƒì„±\\n\")\n",
    "\n",
    "        return split_documents\n",
    "\n",
    "    def _initialize_vectorstore(self):\n",
    "        \"\"\"VectorStore ë° EnsembleRetriever ì´ˆê¸°í™”\"\"\"\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"VectorStore ì´ˆê¸°í™”\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        # ê¸°ì¡´ ChromaDB í™•ì¸\n",
    "        if os.path.exists(self.chroma_persist_dir) and os.path.isdir(self.chroma_persist_dir):\n",
    "            print(\"ğŸ“‚ ê¸°ì¡´ VectorStore ë°œê²¬ - ë¡œë“œ ì¤‘...\")\n",
    "            self.vectorstore = Chroma(\n",
    "                collection_name=self.chroma_collection_name,\n",
    "                embedding_function=self.embeddings,\n",
    "                persist_directory=self.chroma_persist_dir\n",
    "            )\n",
    "            print(\"âœ“ VectorStore ë¡œë“œ ì™„ë£Œ\\n\")\n",
    "            print(\"PDF ë¬¸ì„œ ë¡œë“œ ì¤‘ (BM25 ì¸ë±ìŠ¤ìš©)...\")\n",
    "            self.pdf_documents = self._load_pdf_documents()\n",
    "        else:\n",
    "            print(\"ğŸ†• ê¸°ì¡´ VectorStore ì—†ìŒ - ìƒˆë¡œ ìƒì„±\")\n",
    "            print(\"PDF ë¬¸ì„œ ë¡œë“œ ì¤‘...\")\n",
    "            self.pdf_documents = self._load_pdf_documents()\n",
    "\n",
    "            print(\"VectorStore ìƒì„± ì¤‘ (ì„ë² ë”© ìƒì„± - ìˆ˜ ë¶„ ì†Œìš” ê°€ëŠ¥)...\")\n",
    "            self.vectorstore = Chroma.from_documents(\n",
    "                documents=self.pdf_documents,\n",
    "                embedding=self.embeddings,\n",
    "                collection_name=self.chroma_collection_name,\n",
    "                persist_directory=self.chroma_persist_dir\n",
    "            )\n",
    "            print(\"âœ“ VectorStore ìƒì„± ì™„ë£Œ\\n\")\n",
    "\n",
    "        # EnsembleRetriever êµ¬ì„±\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"EnsembleRetriever êµ¬ì„± ì¤‘...\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        bm25_retriever = BM25Retriever.from_documents(self.pdf_documents)\n",
    "        bm25_retriever.k = 5\n",
    "\n",
    "        semantic_retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 5}\n",
    "        )\n",
    "\n",
    "        self.ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=[bm25_retriever, semantic_retriever],\n",
    "            weights=[0.5, 0.5]\n",
    "        )\n",
    "\n",
    "        print(\"âœ“ BM25Retriever ìƒì„± ì™„ë£Œ (k=5)\")\n",
    "        print(\"âœ“ SemanticRetriever ìƒì„± ì™„ë£Œ (k=5)\")\n",
    "        print(\"âœ“ EnsembleRetriever ìƒì„± ì™„ë£Œ (weights=[0.5, 0.5])\\n\")\n",
    "\n",
    "    def _crawl_with_tavily(self, startup_name: str, max_results: int = 5) -> str:\n",
    "        \"\"\"Tavily APIë¥¼ ì‚¬ìš©í•œ ì›¹ ê²€ìƒ‰\"\"\"\n",
    "        api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"TAVILY_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "        print(f\"  [Tavily] ê²€ìƒ‰ ì¤‘...\")\n",
    "        client = TavilyClient(api_key=api_key)\n",
    "\n",
    "        queries = [\n",
    "            f\"{startup_name} ìŠ¤íƒ€íŠ¸ì—… ê¸°ìˆ  í˜ì‹ \",\n",
    "            f\"{startup_name} AI íˆ¬ì ë¹„ì¦ˆë‹ˆìŠ¤\"\n",
    "        ]\n",
    "\n",
    "        collected_text = []\n",
    "\n",
    "        for idx, query in enumerate(queries, 1):\n",
    "            try:\n",
    "                print(f\"    ì¿¼ë¦¬ {idx}: {query}\")\n",
    "\n",
    "                response = client.search(\n",
    "                    query=query,\n",
    "                    search_depth=\"basic\",\n",
    "                    max_results=max_results,\n",
    "                    include_answer=True,\n",
    "                    include_raw_content=False,\n",
    "                    include_domains=None,\n",
    "                    days=365\n",
    "                )\n",
    "\n",
    "                if response.get('answer'):\n",
    "                    collected_text.append(f\"[AI ìš”ì•½] {response['answer']}\")\n",
    "                    print(f\"      âœ“ AI ìš”ì•½ ìˆ˜ì§‘\")\n",
    "\n",
    "                results = response.get('results', [])\n",
    "                print(f\"      âœ“ {len(results)}ê°œ ì¶œì²˜ ë°œê²¬\")\n",
    "\n",
    "                for result in results:\n",
    "                    title = result.get('title', '')\n",
    "                    content = result.get('content', '')\n",
    "                    url = result.get('url', '')\n",
    "                    score = result.get('score', 0)\n",
    "\n",
    "                    if content and len(content) > 50:\n",
    "                        collected_text.append(\n",
    "                            f\"[ì¶œì²˜: {url}]\\nì œëª©: {title}\\në‚´ìš©: {content}\\nê´€ë ¨ì„±: {score:.2f}\"\n",
    "                        )\n",
    "\n",
    "                time.sleep(0.3)\n",
    "            except Exception as e:\n",
    "                print(f\"      âœ— ì¿¼ë¦¬ ì‹¤íŒ¨: {str(e)[:50]}\")\n",
    "                continue\n",
    "\n",
    "        if not collected_text:\n",
    "            raise ValueError(\"Tavilyì—ì„œ ìœ ì˜ë¯¸í•œ ê²°ê³¼ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "        result_text = \"\\n\\n\".join(collected_text)\n",
    "        print(f\"    ì´ {len(collected_text)}ê°œ í•­ëª© ìˆ˜ì§‘\")\n",
    "        return result_text\n",
    "\n",
    "    def _crawl_startup_info(self, startup_name: str, max_results: int = 5) -> str:\n",
    "        \"\"\"ìŠ¤íƒ€íŠ¸ì—… ì •ë³´ë¥¼ ì›¹ì—ì„œ í¬ë¡¤ë§\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ì›¹ ê²€ìƒ‰ ì‹œì‘: {startup_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        try:\n",
    "            result = self._crawl_with_tavily(startup_name, max_results)\n",
    "            if result and len(result) > 100:\n",
    "                print(f\"âœ“ Tavily ê²€ìƒ‰ ì„±ê³µ\\n\")\n",
    "                return result\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Tavily ì˜¤ë¥˜: {str(e)[:50]}\")\n",
    "            return f\"ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}\"\n",
    "\n",
    "    def _build_workflow(self):\n",
    "        \"\"\"LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±\"\"\"\n",
    "\n",
    "        def select_next_startup(state: TechState) -> TechState:\n",
    "            \"\"\"ë‹¤ìŒ í‰ê°€í•  ìŠ¤íƒ€íŠ¸ì—… ì„ íƒ\"\"\"\n",
    "            idx = state.get(\"processing_index\", 0)\n",
    "\n",
    "            if idx < len(state[\"startup_names\"]):\n",
    "                state[\"current_startup\"] = state[\"startup_names\"][idx]\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"[{idx+1}/{len(state['startup_names'])}] {state['current_startup']} í‰ê°€ ì‹œì‘\")\n",
    "                print(f\"{'='*60}\")\n",
    "\n",
    "            return state\n",
    "\n",
    "        def crawl_web_data(state: TechState) -> TechState:\n",
    "            \"\"\"ì›¹ì—ì„œ ìŠ¤íƒ€íŠ¸ì—… ì •ë³´ í¬ë¡¤ë§\"\"\"\n",
    "            startup_name = state[\"current_startup\"]\n",
    "            web_data = self._crawl_startup_info(startup_name, max_results=5)\n",
    "            state[\"web_data\"] = web_data\n",
    "            return state\n",
    "\n",
    "        def retrieve_tech_info(state: TechState) -> TechState:\n",
    "            \"\"\"PDF ë¬¸ì„œì—ì„œ ê´€ë ¨ ê¸°ìˆ  ì •ë³´ ê²€ìƒ‰\"\"\"\n",
    "            startup_name = state[\"current_startup\"]\n",
    "            query = f\"{startup_name} AI ê¸°ìˆ  í˜ì‹  ìŠ¤íƒ€íŠ¸ì—… íˆ¬ì í‰ê°€ ê²½ìŸë ¥\"\n",
    "\n",
    "            print(f\"\\nPDF ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ ì¤‘...\")\n",
    "            retrieved_docs = self.ensemble_retriever.get_relevant_documents(query)\n",
    "\n",
    "            state[\"retrieved_docs\"] = retrieved_docs\n",
    "            print(f\"ê²€ìƒ‰ ì™„ë£Œ: {len(retrieved_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨\")\n",
    "\n",
    "            return state\n",
    "\n",
    "        def evaluate_technology(state: TechState) -> TechState:\n",
    "            \"\"\"ì›¹ ë°ì´í„°ì™€ PDF ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¸°ìˆ ë ¥ í‰ê°€\"\"\"\n",
    "            startup_name = state[\"current_startup\"]\n",
    "            web_data = state.get(\"web_data\", \"ì •ë³´ ì—†ìŒ\")\n",
    "            docs = state[\"retrieved_docs\"]\n",
    "            current_index = state.get(\"processing_index\", 0)\n",
    "\n",
    "            existing_evaluations = state.get(\"tech_evaluations\", [])\n",
    "            existing_scores = [e['ê¸°ìˆ _ì ìˆ˜'] for e in existing_evaluations if isinstance(e, dict)]\n",
    "\n",
    "            pdf_context = \"\\n\\n\".join([doc.page_content for doc in docs[:3]])\n",
    "\n",
    "            print(f\"\\nGPT-4o-minië¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ìˆ ë ¥ í‰ê°€ ì¤‘...\")\n",
    "            print(f\"  í˜„ì¬ê¹Œì§€ í‰ê°€ ì™„ë£Œ: {len(existing_evaluations)}ê°œ\")\n",
    "\n",
    "            existing_scores_constraint = \"\"\n",
    "            if existing_scores:\n",
    "                scores_str = \", \".join(str(s) for s in existing_scores)\n",
    "                existing_scores_constraint = f\"\"\"\n",
    "### âš ï¸ ì¤‘ìš”í•œ ì œì•½ ì¡°ê±´ âš ï¸\n",
    "ì´ë¯¸ í‰ê°€í•œ ê¸°ì—…ë“¤ì˜ ì ìˆ˜: [{scores_str}]\n",
    "\n",
    "**í•„ìˆ˜**: ìƒˆë¡œìš´ ê¸°ìˆ _ì ìˆ˜ëŠ” ìœ„ ì ìˆ˜ë“¤ê³¼ **ìµœì†Œ 5ì  ì´ìƒ ì°¨ì´**ê°€ ë‚˜ì•¼ í•©ë‹ˆë‹¤.\n",
    "- ì´ë¯¸ ì‚¬ìš©ëœ ì ìˆ˜: {existing_scores}\n",
    "- ì‚¬ìš© ê¸ˆì§€ ë²”ìœ„: {', '.join(f'{s}Â±4ì ' for s in existing_scores)}\n",
    "- ê° ê¸°ì—…ì˜ ì‹¤ì œ ê°•ì ê³¼ ì•½ì ì„ ë°˜ì˜í•˜ì—¬ ì°¨ë³„í™”ëœ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ì„¸ìš”.\n",
    "\"\"\"\n",
    "\n",
    "            eval_prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", \"\"\"ë‹¹ì‹ ì€ AI ìŠ¤íƒ€íŠ¸ì—… íˆ¬ì ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "ì£¼ì–´ì§„ ì›¹ ì •ë³´ì™€ ì—…ê³„ íŠ¸ë Œë“œ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ìŠ¤íƒ€íŠ¸ì—…ì˜ ê¸°ìˆ ë ¥ì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.\n",
    "\n",
    "## í‰ê°€ ê¸°ì¤€ (ë‹¨ê³„ë³„ í‰ê°€):\n",
    "\n",
    "**1ë‹¨ê³„: ê° í•­ëª©ë³„ ì ìˆ˜ ì‚°ì •**\n",
    "- ê¸°ìˆ ì˜ í˜ì‹ ì„± (0-30ì ): AI ê¸°ìˆ ì˜ ë…ì°½ì„±, ì°¨ë³„í™”ëœ ì ‘ê·¼ ë°©ì‹\n",
    "- ê¸°ìˆ ì˜ ì™„ì„±ë„ (0-30ì ): ì œí’ˆ/ì„œë¹„ìŠ¤ì˜ ì™„ì„±ë„, ì‹¤ì œ ì ìš© ì‚¬ë¡€\n",
    "- ì‹œì¥ ê²½ìŸë ¥ (0-20ì ): ê²½ìŸì‚¬ ëŒ€ë¹„ ìš°ìœ„, ì‹œì¥ í¬ì§€ì…”ë‹\n",
    "- íŠ¹í—ˆ/ì§€ì‹ì¬ì‚°ê¶Œ (0-10ì ): íŠ¹í—ˆ, ë…¼ë¬¸, ê¸°ìˆ  ìì‚°\n",
    "- ê¸°ìˆ  í™•ì¥ ê°€ëŠ¥ì„± (0-10ì ): ìŠ¤ì¼€ì¼ì—… ê°€ëŠ¥ì„±, ë‹¤ë¥¸ ë¶„ì•¼ ì ìš©\n",
    "\n",
    "**2ë‹¨ê³„: ì´ì  ê³„ì‚°**\n",
    "ìœ„ 5ê°œ í•­ëª©ì˜ ì ìˆ˜ë¥¼ í•©ì‚°í•˜ì—¬ ìµœì¢… ê¸°ìˆ _ì ìˆ˜ë¥¼ ë„ì¶œí•˜ì„¸ìš”.\n",
    "\n",
    "**ì¤‘ìš”**:\n",
    "- ê¸°ì—…ë§ˆë‹¤ ëª…í™•íˆ ì°¨ë³„í™”ëœ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ì„¸ìš”\n",
    "- ëª¨ë“  ê¸°ì—…ì—ê²Œ ë¹„ìŠ·í•œ ì ìˆ˜ë¥¼ ì£¼ì§€ ë§ˆì„¸ìš”\n",
    "- ê° ê¸°ì—…ì˜ ì‹¤ì œ ê°•ì ê³¼ ì•½ì ì„ ì •í™•íˆ ë°˜ì˜í•˜ì„¸ìš”\"\"\"),\n",
    "                (\"user\", \"\"\"ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„: {startup_name}\n",
    "\n",
    "=== ì›¹ì—ì„œ ìˆ˜ì§‘í•œ ì •ë³´ ===\n",
    "{web_data}\n",
    "\n",
    "=== ì—…ê³„ íŠ¸ë Œë“œ ë° ì°¸ê³  ë¬¸ì„œ ===\n",
    "{pdf_context}\n",
    "\n",
    "{existing_scores_constraint}\n",
    "\n",
    "ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ë‹¨ê³„ë³„ë¡œ í‰ê°€**í•˜ê³  ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì‘ì„±í•˜ì„¸ìš”:\n",
    "\n",
    "{{\n",
    "    \"startup_name\": \"ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„\",\n",
    "    \"í•­ëª©ë³„_ì ìˆ˜\": {{\n",
    "        \"í˜ì‹ ì„±\": ì ìˆ˜ (0-30),\n",
    "        \"ì™„ì„±ë„\": ì ìˆ˜ (0-30),\n",
    "        \"ê²½ìŸë ¥\": ì ìˆ˜ (0-20),\n",
    "        \"íŠ¹í—ˆ\": ì ìˆ˜ (0-10),\n",
    "        \"í™•ì¥ì„±\": ì ìˆ˜ (0-10)\n",
    "    }},\n",
    "    \"ê¸°ìˆ _ì ìˆ˜\": ì´ì  (0-100, ì •ìˆ˜),\n",
    "    \"ê¸°ìˆ _ë¶„ì„_ê·¼ê±°\": \"ê° í•­ëª©ë³„ ì ìˆ˜ ì‚°ì • ì´ìœ ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…. í˜ì‹ ì„±, ì™„ì„±ë„, ê²½ìŸë ¥, íŠ¹í—ˆ, í™•ì¥ì„± ê°ê°ì— ëŒ€í•´ ì›¹ ì •ë³´ë¥¼ ì¸ìš©í•˜ì—¬ ìƒì„¸íˆ ë¶„ì„\"\n",
    "}}\n",
    "\n",
    "**í•„ìˆ˜**: ê¸°ìˆ _ì ìˆ˜ëŠ” í•­ëª©ë³„_ì ìˆ˜ì˜ í•©ê³¼ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.\"\"\")\n",
    "            ])\n",
    "\n",
    "            chain = eval_prompt | self.llm\n",
    "            response = chain.invoke({\n",
    "                \"startup_name\": startup_name,\n",
    "                \"web_data\": web_data[:2000],\n",
    "                \"pdf_context\": pdf_context[:3000],\n",
    "                \"existing_scores_constraint\": existing_scores_constraint\n",
    "            })\n",
    "\n",
    "            try:\n",
    "                content = response.content\n",
    "                if \"```json\" in content:\n",
    "                    content = content.split(\"```json\")[1].split(\"```\")[0]\n",
    "                elif \"```\" in content:\n",
    "                    content = content.split(\"```\")[1].split(\"```\")[0]\n",
    "\n",
    "                evaluation = json.loads(content.strip())\n",
    "\n",
    "                if \"í•­ëª©ë³„_ì ìˆ˜\" in evaluation:\n",
    "                    item_scores = evaluation[\"í•­ëª©ë³„_ì ìˆ˜\"]\n",
    "                    calculated_total = sum(item_scores.values())\n",
    "                    reported_total = evaluation.get(\"ê¸°ìˆ _ì ìˆ˜\", calculated_total)\n",
    "\n",
    "                    if abs(calculated_total - reported_total) > 1:\n",
    "                        print(f\"  âš ï¸ ì ìˆ˜ ë¶ˆì¼ì¹˜ ê°ì§€ (ë³´ê³ : {reported_total}, ê³„ì‚°: {calculated_total}) - ì¬ê³„ì‚°ëœ ê°’ ì‚¬ìš©\")\n",
    "                        evaluation[\"ê¸°ìˆ _ì ìˆ˜\"] = calculated_total\n",
    "\n",
    "                print(f\"í‰ê°€ ì™„ë£Œ: {evaluation['ê¸°ìˆ _ì ìˆ˜']}ì \")\n",
    "\n",
    "                if \"í•­ëª©ë³„_ì ìˆ˜\" in evaluation:\n",
    "                    scores_breakdown = \", \".join([f\"{k}={v}\" for k, v in evaluation[\"í•­ëª©ë³„_ì ìˆ˜\"].items()])\n",
    "                    print(f\"  ì„¸ë¶€: {scores_breakdown}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"JSON íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "                evaluation = {\n",
    "                    \"startup_name\": startup_name,\n",
    "                    \"ê¸°ìˆ _ì ìˆ˜\": 50,\n",
    "                    \"ê¸°ìˆ _ë¶„ì„_ê·¼ê±°\": f\"í‰ê°€ ì‹¤íŒ¨: {str(e)}\"\n",
    "                }\n",
    "\n",
    "            current_evaluations = state.get(\"tech_evaluations\", [])\n",
    "            current_evaluations.append(evaluation)\n",
    "            state[\"tech_evaluations\"] = current_evaluations\n",
    "            state[\"processing_index\"] = current_index + 1\n",
    "\n",
    "            print(f\"ì§„í–‰ ìƒí™©: {state['processing_index']}/{len(state['startup_names'])} ì™„ë£Œ\")\n",
    "            print(f\"  ëˆ„ì  í‰ê°€ ê²°ê³¼: {len(state['tech_evaluations'])}ê°œ\\n\")\n",
    "\n",
    "            return state\n",
    "\n",
    "        def check_completion(state: TechState) -> str:\n",
    "            \"\"\"ëª¨ë“  ìŠ¤íƒ€íŠ¸ì—… í‰ê°€ ì™„ë£Œ ì—¬ë¶€ í™•ì¸\"\"\"\n",
    "            idx = state.get(\"processing_index\", 0)\n",
    "            total = len(state.get(\"startup_names\", []))\n",
    "\n",
    "            if idx < total:\n",
    "                return \"continue\"\n",
    "            else:\n",
    "                return \"end\"\n",
    "\n",
    "        # StateGraph ìƒì„±\n",
    "        workflow = StateGraph(TechState)\n",
    "\n",
    "        # ë…¸ë“œ ì¶”ê°€\n",
    "        workflow.add_node(\"select_startup\", select_next_startup)\n",
    "        workflow.add_node(\"crawl_web\", crawl_web_data)\n",
    "        workflow.add_node(\"retrieve_info\", retrieve_tech_info)\n",
    "        workflow.add_node(\"evaluate\", evaluate_technology)\n",
    "\n",
    "        # ì—£ì§€ ì„¤ì •\n",
    "        workflow.set_entry_point(\"select_startup\")\n",
    "        workflow.add_edge(\"select_startup\", \"crawl_web\")\n",
    "        workflow.add_edge(\"crawl_web\", \"retrieve_info\")\n",
    "        workflow.add_edge(\"retrieve_info\", \"evaluate\")\n",
    "\n",
    "        # ì¡°ê±´ë¶€ ì—£ì§€\n",
    "        workflow.add_conditional_edges(\n",
    "            \"evaluate\",\n",
    "            check_completion,\n",
    "            {\n",
    "                \"continue\": \"select_startup\",\n",
    "                \"end\": END\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "        self.app = workflow.compile()\n",
    "\n",
    "        print(\"\\nì›Œí¬í”Œë¡œìš° êµ¬ì„± ì™„ë£Œ!\")\n",
    "        print(\"ìˆœì„œ: select_startup -> crawl_web -> retrieve_info -> evaluate -> [ë°˜ë³µ or ì¢…ë£Œ]\\n\")\n",
    "\n",
    "    def get_tech_result(self) -> Dict:\n",
    "        \"\"\"\n",
    "        ê¸°ìˆ  í‰ê°€ ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜\n",
    "\n",
    "        Returns:\n",
    "            í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬\n",
    "            {\n",
    "                \"tech_evaluations\": [...],  # ê° ìŠ¤íƒ€íŠ¸ì—…ë³„ í‰ê°€ ê²°ê³¼\n",
    "                \"summary\": {...}  # ìš”ì•½ í†µê³„\n",
    "            }\n",
    "        \"\"\"\n",
    "        # VectorStore ì´ˆê¸°í™” (ì•„ì§ ì•ˆ ë˜ì–´ ìˆìœ¼ë©´)\n",
    "        if self.vectorstore is None:\n",
    "            self._initialize_vectorstore()\n",
    "\n",
    "        # Workflow êµ¬ì„± (ì•„ì§ ì•ˆ ë˜ì–´ ìˆìœ¼ë©´)\n",
    "        if self.app is None:\n",
    "            self._build_workflow()\n",
    "\n",
    "        # ì´ˆê¸° ìƒíƒœ ì„¤ì •\n",
    "        initial_state = {\n",
    "            \"startup_names\": self.startup_names,\n",
    "            \"current_startup\": \"\",\n",
    "            \"web_data\": \"\",\n",
    "            \"retrieved_docs\": [],\n",
    "            \"tech_evaluations\": [],\n",
    "            \"processing_index\": 0,\n",
    "            \"vectorstore_ready\": True\n",
    "        }\n",
    "\n",
    "        # ì—ì´ì „íŠ¸ ì‹¤í–‰\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"AI ìŠ¤íƒ€íŠ¸ì—… ê¸°ìˆ  í‰ê°€ ì—ì´ì „íŠ¸ ì‹œì‘\")\n",
    "        print(f\"í‰ê°€ ëŒ€ìƒ: {len(self.startup_names)}ê°œ ê¸°ì—…\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        result = self.app.invoke(initial_state)\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ì „ì²´ í‰ê°€ ì™„ë£Œ\")\n",
    "        print(f\"ìµœì¢… í‰ê°€ ê²°ê³¼ ìˆ˜: {len(result['tech_evaluations'])}ê°œ\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "\n",
    "        return result[\"tech_evaluations\"]\n",
    "\n",
    "# # ì‚¬ìš© ì˜ˆì‹œ\n",
    "if __name__ == \"__main__\":\n",
    "    # ë‹¨ì¼ ê¸°ì—… í‰ê°€\n",
    "    company_name = \"ì–´ë”©\"\n",
    "    agent = TechAgent(startups_to_evaluate=company_name)\n",
    "    result = agent.get_tech_result().pop()\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b2b70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-5k19pLvO-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
