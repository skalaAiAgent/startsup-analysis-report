"""
AI ìŠ¤íƒ€íŠ¸ì—… ê¸°ìˆ  í‰ê°€ ì—ì´ì „íŠ¸
Langgraphë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ í¬ë¡¤ë§ + PDF ë¶„ì„ì„ í†µí•œ ê¸°ìˆ ë ¥ í‰ê°€
"""

import os
import sys
from pathlib import Path
from typing import TypedDict, List, Dict
import json
import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
ROOT_DIR = os.path.normpath(os.path.join(os.path.dirname(__file__), os.pardir))
if ROOT_DIR not in sys.path:
    sys.path.insert(0, ROOT_DIR)

from langchain_openai import ChatOpenAI
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.retrievers import BM25Retriever
from langchain_classic.retrievers import EnsembleRetriever
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, END
from dotenv import load_dotenv
from tavily import TavilyClient

load_dotenv()


class TechState(TypedDict):
    """ì—ì´ì „íŠ¸ì˜ ìƒíƒœë¥¼ ì •ì˜í•˜ëŠ” í´ë˜ìŠ¤"""
    startup_names: List[str]
    current_startup: str
    web_data: str
    retrieved_docs: List[Document]
    tech_evaluations: List[Dict]
    processing_index: int
    vectorstore_ready: bool


class TechAgent:
    """
    AI ìŠ¤íƒ€íŠ¸ì—… ê¸°ìˆ  í‰ê°€ ì—ì´ì „íŠ¸

    ì‚¬ìš©ë²•:
        agent = TechAgent(startups_to_evaluate="ì–´ë”©")
        result = agent.get_tech_result()
    """

    def __init__(self, startups_to_evaluate: str | List[str], pdf_data_path: str = None):
        """
        Args:
            startups_to_evaluate: í‰ê°€í•  ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„ (ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸)
            pdf_data_path: PDF ë°ì´í„° ê²½ë¡œ (ê¸°ë³¸ê°’: í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ data í´ë”)
        """
        # ìŠ¤íƒ€íŠ¸ì—… ë¦¬ìŠ¤íŠ¸ ì„¤ì •
        if isinstance(startups_to_evaluate, str):
            self.startup_names = [startups_to_evaluate]
        else:
            self.startup_names = startups_to_evaluate

        # PDF ë°ì´í„° ê²½ë¡œ ì„¤ì •
        if pdf_data_path is None:
            # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ data í´ë” ì‚¬ìš©
            self.pdf_data_path = Path(ROOT_DIR).parent / "data"
        else:
            self.pdf_data_path = Path(pdf_data_path)

        # ëª¨ë¸ ì´ˆê¸°í™”
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)
        self.embeddings = OllamaEmbeddings(model="nomic-embed-text")

        # ChromaDB ê²½ë¡œ
        self.chroma_persist_dir = os.path.join(ROOT_DIR, "rag", "tech", "chroma_db")
        self.chroma_collection_name = "startup_tech_db"

        # VectorStore ë° Retriever ì´ˆê¸°í™”
        self.vectorstore = None
        self.ensemble_retriever = None
        self.pdf_documents = None

        # Workflow ì´ˆê¸°í™”
        self.app = None

        print(f"\n{'='*60}")
        print(f"TechAgent ì´ˆê¸°í™”")
        print(f"{'='*60}")
        print(f"í‰ê°€ ëŒ€ìƒ: {', '.join(self.startup_names)}")
        print(f"PDF ê²½ë¡œ: {self.pdf_data_path}")
        print(f"{'='*60}\n")

    def _load_pdf_documents(self) -> List[Document]:
        """PDF ë¬¸ì„œë“¤ì„ ë¡œë“œí•˜ê³  ì²­í‚¹"""
        all_documents = []

        pdf_files = list(self.pdf_data_path.glob("*.pdf"))
        print(f"ë°œê²¬ëœ PDF íŒŒì¼: {len(pdf_files)}ê°œ")

        for pdf_file in pdf_files:
            try:
                print(f"  ë¡œë”© ì¤‘: {pdf_file.name}")
                loader = PyPDFLoader(str(pdf_file))
                documents = loader.load()

                for doc in documents:
                    doc.metadata["source_file"] = pdf_file.name
                    doc.metadata["source_type"] = "pdf"

                all_documents.extend(documents)
            except Exception as e:
                print(f"  PDF ë¡œë“œ ì‹¤íŒ¨ ({pdf_file.name}): {e}")

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )

        split_documents = text_splitter.split_documents(all_documents)
        print(f"ì´ {len(split_documents)}ê°œì˜ ì²­í¬ ìƒì„±\n")

        return split_documents

    def _initialize_vectorstore(self):
        """VectorStore ë° EnsembleRetriever ì´ˆê¸°í™”"""
        print(f"{'='*60}")
        print(f"VectorStore ì´ˆê¸°í™”")
        print(f"{'='*60}\n")

        # ê¸°ì¡´ ChromaDB í™•ì¸
        if os.path.exists(self.chroma_persist_dir) and os.path.isdir(self.chroma_persist_dir):
            print("ğŸ“‚ ê¸°ì¡´ VectorStore ë°œê²¬ - ë¡œë“œ ì¤‘...")
            self.vectorstore = Chroma(
                collection_name=self.chroma_collection_name,
                embedding_function=self.embeddings,
                persist_directory=self.chroma_persist_dir
            )
            print("âœ“ VectorStore ë¡œë“œ ì™„ë£Œ\n")

            print("PDF ë¬¸ì„œ ë¡œë“œ ì¤‘ (BM25 ì¸ë±ìŠ¤ìš©)...")
            self.pdf_documents = self._load_pdf_documents()
        else:
            print("ğŸ†• ê¸°ì¡´ VectorStore ì—†ìŒ - ìƒˆë¡œ ìƒì„±")
            print("PDF ë¬¸ì„œ ë¡œë“œ ì¤‘...")
            self.pdf_documents = self._load_pdf_documents()

            print("VectorStore ìƒì„± ì¤‘ (ì„ë² ë”© ìƒì„± - ìˆ˜ ë¶„ ì†Œìš” ê°€ëŠ¥)...")
            self.vectorstore = Chroma.from_documents(
                documents=self.pdf_documents,
                embedding=self.embeddings,
                collection_name=self.chroma_collection_name,
                persist_directory=self.chroma_persist_dir
            )
            print("âœ“ VectorStore ìƒì„± ì™„ë£Œ\n")

        # EnsembleRetriever êµ¬ì„±
        print(f"{'='*60}")
        print(f"EnsembleRetriever êµ¬ì„± ì¤‘...")
        print(f"{'='*60}\n")

        bm25_retriever = BM25Retriever.from_documents(self.pdf_documents)
        bm25_retriever.k = 5

        semantic_retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 5}
        )

        self.ensemble_retriever = EnsembleRetriever(
            retrievers=[bm25_retriever, semantic_retriever],
            weights=[0.5, 0.5]
        )

        print("âœ“ BM25Retriever ìƒì„± ì™„ë£Œ (k=5)")
        print("âœ“ SemanticRetriever ìƒì„± ì™„ë£Œ (k=5)")
        print("âœ“ EnsembleRetriever ìƒì„± ì™„ë£Œ (weights=[0.5, 0.5])\n")

    def _crawl_with_tavily(self, startup_name: str, max_results: int = 5) -> str:
        """Tavily APIë¥¼ ì‚¬ìš©í•œ ì›¹ ê²€ìƒ‰"""
        api_key = os.getenv("TAVILY_API_KEY")
        if not api_key:
            raise ValueError("TAVILY_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        print(f"  [Tavily] ê²€ìƒ‰ ì¤‘...")
        client = TavilyClient(api_key=api_key)

        queries = [
            f"{startup_name} ìŠ¤íƒ€íŠ¸ì—… ê¸°ìˆ  í˜ì‹ ",
            f"{startup_name} AI íˆ¬ì ë¹„ì¦ˆë‹ˆìŠ¤"
        ]

        collected_text = []

        for idx, query in enumerate(queries, 1):
            try:
                print(f"    ì¿¼ë¦¬ {idx}: {query}")

                response = client.search(
                    query=query,
                    search_depth="basic",
                    max_results=max_results,
                    include_answer=True,
                    include_raw_content=False,
                    include_domains=None,
                    days=365
                )

                if response.get('answer'):
                    collected_text.append(f"[AI ìš”ì•½] {response['answer']}")
                    print(f"      âœ“ AI ìš”ì•½ ìˆ˜ì§‘")

                results = response.get('results', [])
                print(f"      âœ“ {len(results)}ê°œ ì¶œì²˜ ë°œê²¬")

                for result in results:
                    title = result.get('title', '')
                    content = result.get('content', '')
                    url = result.get('url', '')
                    score = result.get('score', 0)

                    if content and len(content) > 50:
                        collected_text.append(
                            f"[ì¶œì²˜: {url}]\nì œëª©: {title}\në‚´ìš©: {content}\nê´€ë ¨ì„±: {score:.2f}"
                        )

                time.sleep(0.3)
            except Exception as e:
                print(f"      âœ— ì¿¼ë¦¬ ì‹¤íŒ¨: {str(e)[:50]}")
                continue

        if not collected_text:
            raise ValueError("Tavilyì—ì„œ ìœ ì˜ë¯¸í•œ ê²°ê³¼ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        result_text = "\n\n".join(collected_text)
        print(f"    ì´ {len(collected_text)}ê°œ í•­ëª© ìˆ˜ì§‘")
        return result_text

    def _crawl_startup_info(self, startup_name: str, max_results: int = 5) -> str:
        """ìŠ¤íƒ€íŠ¸ì—… ì •ë³´ë¥¼ ì›¹ì—ì„œ í¬ë¡¤ë§"""
        print(f"\n{'='*60}")
        print(f"ì›¹ ê²€ìƒ‰ ì‹œì‘: {startup_name}")
        print(f"{'='*60}")

        try:
            result = self._crawl_with_tavily(startup_name, max_results)
            if result and len(result) > 100:
                print(f"âœ“ Tavily ê²€ìƒ‰ ì„±ê³µ\n")
                return result
        except Exception as e:
            print(f"âœ— Tavily ì˜¤ë¥˜: {str(e)[:50]}")
            return f"ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"

    def _build_workflow(self):
        """LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±"""

        def select_next_startup(state: TechState) -> TechState:
            """ë‹¤ìŒ í‰ê°€í•  ìŠ¤íƒ€íŠ¸ì—… ì„ íƒ"""
            idx = state.get("processing_index", 0)

            if idx < len(state["startup_names"]):
                state["current_startup"] = state["startup_names"][idx]
                print(f"\n{'='*60}")
                print(f"[{idx+1}/{len(state['startup_names'])}] {state['current_startup']} í‰ê°€ ì‹œì‘")
                print(f"{'='*60}")

            return state

        def crawl_web_data(state: TechState) -> TechState:
            """ì›¹ì—ì„œ ìŠ¤íƒ€íŠ¸ì—… ì •ë³´ í¬ë¡¤ë§"""
            startup_name = state["current_startup"]
            web_data = self._crawl_startup_info(startup_name, max_results=5)
            state["web_data"] = web_data
            return state

        def retrieve_tech_info(state: TechState) -> TechState:
            """PDF ë¬¸ì„œì—ì„œ ê´€ë ¨ ê¸°ìˆ  ì •ë³´ ê²€ìƒ‰"""
            startup_name = state["current_startup"]
            query = f"{startup_name} AI ê¸°ìˆ  í˜ì‹  ìŠ¤íƒ€íŠ¸ì—… íˆ¬ì í‰ê°€ ê²½ìŸë ¥"

            print(f"\nPDF ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ ì¤‘...")
            retrieved_docs = self.ensemble_retriever.get_relevant_documents(query)

            state["retrieved_docs"] = retrieved_docs
            print(f"ê²€ìƒ‰ ì™„ë£Œ: {len(retrieved_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")

            return state

        def evaluate_technology(state: TechState) -> TechState:
            """ì›¹ ë°ì´í„°ì™€ PDF ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¸°ìˆ ë ¥ í‰ê°€"""
            startup_name = state["current_startup"]
            web_data = state.get("web_data", "ì •ë³´ ì—†ìŒ")
            docs = state["retrieved_docs"]
            current_index = state.get("processing_index", 0)

            existing_evaluations = state.get("tech_evaluations", [])
            existing_scores = [e['ê¸°ìˆ _ì ìˆ˜'] for e in existing_evaluations if isinstance(e, dict)]

            pdf_context = "\n\n".join([doc.page_content for doc in docs[:3]])

            print(f"\nGPT-4o-minië¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ìˆ ë ¥ í‰ê°€ ì¤‘...")
            print(f"  í˜„ì¬ê¹Œì§€ í‰ê°€ ì™„ë£Œ: {len(existing_evaluations)}ê°œ")

            existing_scores_constraint = ""
            if existing_scores:
                scores_str = ", ".join(str(s) for s in existing_scores)
                existing_scores_constraint = f"""
### âš ï¸ ì¤‘ìš”í•œ ì œì•½ ì¡°ê±´ âš ï¸
ì´ë¯¸ í‰ê°€í•œ ê¸°ì—…ë“¤ì˜ ì ìˆ˜: [{scores_str}]

**í•„ìˆ˜**: ìƒˆë¡œìš´ ê¸°ìˆ _ì ìˆ˜ëŠ” ìœ„ ì ìˆ˜ë“¤ê³¼ **ìµœì†Œ 5ì  ì´ìƒ ì°¨ì´**ê°€ ë‚˜ì•¼ í•©ë‹ˆë‹¤.
- ì´ë¯¸ ì‚¬ìš©ëœ ì ìˆ˜: {existing_scores}
- ì‚¬ìš© ê¸ˆì§€ ë²”ìœ„: {', '.join(f'{s}Â±4ì ' for s in existing_scores)}
- ê° ê¸°ì—…ì˜ ì‹¤ì œ ê°•ì ê³¼ ì•½ì ì„ ë°˜ì˜í•˜ì—¬ ì°¨ë³„í™”ëœ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ì„¸ìš”.
"""

            eval_prompt = ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ AI ìŠ¤íƒ€íŠ¸ì—… íˆ¬ì ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì›¹ ì •ë³´ì™€ ì—…ê³„ íŠ¸ë Œë“œ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ìŠ¤íƒ€íŠ¸ì—…ì˜ ê¸°ìˆ ë ¥ì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.

## í‰ê°€ ê¸°ì¤€ (ë‹¨ê³„ë³„ í‰ê°€):

**1ë‹¨ê³„: ê° í•­ëª©ë³„ ì ìˆ˜ ì‚°ì •**
- ê¸°ìˆ ì˜ í˜ì‹ ì„± (0-30ì ): AI ê¸°ìˆ ì˜ ë…ì°½ì„±, ì°¨ë³„í™”ëœ ì ‘ê·¼ ë°©ì‹
- ê¸°ìˆ ì˜ ì™„ì„±ë„ (0-30ì ): ì œí’ˆ/ì„œë¹„ìŠ¤ì˜ ì™„ì„±ë„, ì‹¤ì œ ì ìš© ì‚¬ë¡€
- ì‹œì¥ ê²½ìŸë ¥ (0-20ì ): ê²½ìŸì‚¬ ëŒ€ë¹„ ìš°ìœ„, ì‹œì¥ í¬ì§€ì…”ë‹
- íŠ¹í—ˆ/ì§€ì‹ì¬ì‚°ê¶Œ (0-10ì ): íŠ¹í—ˆ, ë…¼ë¬¸, ê¸°ìˆ  ìì‚°
- ê¸°ìˆ  í™•ì¥ ê°€ëŠ¥ì„± (0-10ì ): ìŠ¤ì¼€ì¼ì—… ê°€ëŠ¥ì„±, ë‹¤ë¥¸ ë¶„ì•¼ ì ìš©

**2ë‹¨ê³„: ì´ì  ê³„ì‚°**
ìœ„ 5ê°œ í•­ëª©ì˜ ì ìˆ˜ë¥¼ í•©ì‚°í•˜ì—¬ ìµœì¢… ê¸°ìˆ _ì ìˆ˜ë¥¼ ë„ì¶œí•˜ì„¸ìš”.

**ì¤‘ìš”**:
- ê¸°ì—…ë§ˆë‹¤ ëª…í™•íˆ ì°¨ë³„í™”ëœ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ì„¸ìš”
- ëª¨ë“  ê¸°ì—…ì—ê²Œ ë¹„ìŠ·í•œ ì ìˆ˜ë¥¼ ì£¼ì§€ ë§ˆì„¸ìš”
- ê° ê¸°ì—…ì˜ ì‹¤ì œ ê°•ì ê³¼ ì•½ì ì„ ì •í™•íˆ ë°˜ì˜í•˜ì„¸ìš”"""),
                ("user", """ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„: {startup_name}

=== ì›¹ì—ì„œ ìˆ˜ì§‘í•œ ì •ë³´ ===
{web_data}

=== ì—…ê³„ íŠ¸ë Œë“œ ë° ì°¸ê³  ë¬¸ì„œ ===
{pdf_context}

{existing_scores_constraint}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ë‹¨ê³„ë³„ë¡œ í‰ê°€**í•˜ê³  ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì‘ì„±í•˜ì„¸ìš”:

{{
    "startup_name": "ìŠ¤íƒ€íŠ¸ì—… ì´ë¦„",
    "í•­ëª©ë³„_ì ìˆ˜": {{
        "í˜ì‹ ì„±": ì ìˆ˜ (0-30),
        "ì™„ì„±ë„": ì ìˆ˜ (0-30),
        "ê²½ìŸë ¥": ì ìˆ˜ (0-20),
        "íŠ¹í—ˆ": ì ìˆ˜ (0-10),
        "í™•ì¥ì„±": ì ìˆ˜ (0-10)
    }},
    "ê¸°ìˆ _ì ìˆ˜": ì´ì  (0-100, ì •ìˆ˜),
    "ê¸°ìˆ _ë¶„ì„_ê·¼ê±°": "ê° í•­ëª©ë³„ ì ìˆ˜ ì‚°ì • ì´ìœ ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…. í˜ì‹ ì„±, ì™„ì„±ë„, ê²½ìŸë ¥, íŠ¹í—ˆ, í™•ì¥ì„± ê°ê°ì— ëŒ€í•´ ì›¹ ì •ë³´ë¥¼ ì¸ìš©í•˜ì—¬ ìƒì„¸íˆ ë¶„ì„"
}}

**í•„ìˆ˜**: ê¸°ìˆ _ì ìˆ˜ëŠ” í•­ëª©ë³„_ì ìˆ˜ì˜ í•©ê³¼ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.""")
            ])

            chain = eval_prompt | self.llm
            response = chain.invoke({
                "startup_name": startup_name,
                "web_data": web_data[:2000],
                "pdf_context": pdf_context[:3000],
                "existing_scores_constraint": existing_scores_constraint
            })

            try:
                content = response.content
                if "```json" in content:
                    content = content.split("```json")[1].split("```")[0]
                elif "```" in content:
                    content = content.split("```")[1].split("```")[0]

                evaluation = json.loads(content.strip())

                if "í•­ëª©ë³„_ì ìˆ˜" in evaluation:
                    item_scores = evaluation["í•­ëª©ë³„_ì ìˆ˜"]
                    calculated_total = sum(item_scores.values())
                    reported_total = evaluation.get("ê¸°ìˆ _ì ìˆ˜", calculated_total)

                    if abs(calculated_total - reported_total) > 1:
                        print(f"  âš ï¸ ì ìˆ˜ ë¶ˆì¼ì¹˜ ê°ì§€ (ë³´ê³ : {reported_total}, ê³„ì‚°: {calculated_total}) - ì¬ê³„ì‚°ëœ ê°’ ì‚¬ìš©")
                        evaluation["ê¸°ìˆ _ì ìˆ˜"] = calculated_total

                print(f"í‰ê°€ ì™„ë£Œ: {evaluation['ê¸°ìˆ _ì ìˆ˜']}ì ")

                if "í•­ëª©ë³„_ì ìˆ˜" in evaluation:
                    scores_breakdown = ", ".join([f"{k}={v}" for k, v in evaluation["í•­ëª©ë³„_ì ìˆ˜"].items()])
                    print(f"  ì„¸ë¶€: {scores_breakdown}")

            except Exception as e:
                print(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                evaluation = {
                    "startup_name": startup_name,
                    "ê¸°ìˆ _ì ìˆ˜": 50,
                    "ê¸°ìˆ _ë¶„ì„_ê·¼ê±°": f"í‰ê°€ ì‹¤íŒ¨: {str(e)}"
                }

            current_evaluations = state.get("tech_evaluations", [])
            current_evaluations.append(evaluation)
            state["tech_evaluations"] = current_evaluations
            state["processing_index"] = current_index + 1

            print(f"ì§„í–‰ ìƒí™©: {state['processing_index']}/{len(state['startup_names'])} ì™„ë£Œ")
            print(f"  ëˆ„ì  í‰ê°€ ê²°ê³¼: {len(state['tech_evaluations'])}ê°œ\n")

            return state

        def check_completion(state: TechState) -> str:
            """ëª¨ë“  ìŠ¤íƒ€íŠ¸ì—… í‰ê°€ ì™„ë£Œ ì—¬ë¶€ í™•ì¸"""
            idx = state.get("processing_index", 0)
            total = len(state.get("startup_names", []))

            if idx < total:
                return "continue"
            else:
                return "end"

        # StateGraph ìƒì„±
        workflow = StateGraph(TechState)

        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("select_startup", select_next_startup)
        workflow.add_node("crawl_web", crawl_web_data)
        workflow.add_node("retrieve_info", retrieve_tech_info)
        workflow.add_node("evaluate", evaluate_technology)

        # ì—£ì§€ ì„¤ì •
        workflow.set_entry_point("select_startup")
        workflow.add_edge("select_startup", "crawl_web")
        workflow.add_edge("crawl_web", "retrieve_info")
        workflow.add_edge("retrieve_info", "evaluate")

        # ì¡°ê±´ë¶€ ì—£ì§€
        workflow.add_conditional_edges(
            "evaluate",
            check_completion,
            {
                "continue": "select_startup",
                "end": END
            }
        )

        # ê·¸ë˜í”„ ì»´íŒŒì¼
        self.app = workflow.compile()

        print("\nì›Œí¬í”Œë¡œìš° êµ¬ì„± ì™„ë£Œ!")
        print("ìˆœì„œ: select_startup -> crawl_web -> retrieve_info -> evaluate -> [ë°˜ë³µ or ì¢…ë£Œ]\n")

    def get_tech_result(self) -> Dict:
        """
        ê¸°ìˆ  í‰ê°€ ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜

        Returns:
            í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            {
                "tech_evaluations": [...],  # ê° ìŠ¤íƒ€íŠ¸ì—…ë³„ í‰ê°€ ê²°ê³¼
                "summary": {...}  # ìš”ì•½ í†µê³„
            }
        """
        # VectorStore ì´ˆê¸°í™” (ì•„ì§ ì•ˆ ë˜ì–´ ìˆìœ¼ë©´)
        if self.vectorstore is None:
            self._initialize_vectorstore()

        # Workflow êµ¬ì„± (ì•„ì§ ì•ˆ ë˜ì–´ ìˆìœ¼ë©´)
        if self.app is None:
            self._build_workflow()

        # ì´ˆê¸° ìƒíƒœ ì„¤ì •
        initial_state = {
            "startup_names": self.startup_names,
            "current_startup": "",
            "web_data": "",
            "retrieved_docs": [],
            "tech_evaluations": [],
            "processing_index": 0,
            "vectorstore_ready": True
        }

        # ì—ì´ì „íŠ¸ ì‹¤í–‰
        print(f"\n{'='*60}")
        print(f"AI ìŠ¤íƒ€íŠ¸ì—… ê¸°ìˆ  í‰ê°€ ì—ì´ì „íŠ¸ ì‹œì‘")
        print(f"í‰ê°€ ëŒ€ìƒ: {len(self.startup_names)}ê°œ ê¸°ì—…")
        print(f"{'='*60}")

        result = self.app.invoke(initial_state)

        print(f"\n{'='*60}")
        print(f"ì „ì²´ í‰ê°€ ì™„ë£Œ")
        print(f"ìµœì¢… í‰ê°€ ê²°ê³¼ ìˆ˜: {len(result['tech_evaluations'])}ê°œ")
        print(f"{'='*60}\n")


        return result["tech_evaluations"].pop(), 
